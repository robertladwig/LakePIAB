{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does not make sense\n",
    "\n",
    "Cannot be finetuned with all 4 losses. The term 1 and term 4 are contradictory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ladwi\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layers, activation=\"relu\", init=\"xavier\"):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = torch.nn.Tanh()\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"Unspecified activation type\")\n",
    "        \n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "        if init==\"xavier\":\n",
    "            self.xavier_init_weights()\n",
    "        elif init==\"kaiming\":\n",
    "            self.kaiming_init_weights()\n",
    "    \n",
    "    def xavier_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Xavier Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def kaiming_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Kaiming Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.kaiming_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "    \n",
    "class DataGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          depth  AirTemp_degC  Longwave_Wm-2  Latent_Wm-2  Sensible_Wm-2  \\\n",
       "0            1     -1.252423     584.398073   -21.601480     -26.402849   \n",
       "1            2     -1.252423     584.398073   -21.601480     -26.402849   \n",
       "2            3     -1.252423     584.398073   -21.601480     -26.402849   \n",
       "3            4     -1.252423     584.398073   -21.601480     -26.402849   \n",
       "4            5     -1.252423     584.398073   -21.601480     -26.402849   \n",
       "...        ...           ...            ...          ...            ...   \n",
       "1752345     46     -3.068643     528.790329   -15.910643     -20.374965   \n",
       "1752346     47     -3.068643     528.790329   -15.910643     -20.374965   \n",
       "1752347     48     -3.068643     528.790329   -15.910643     -20.374965   \n",
       "1752348     49     -3.068643     528.790329   -15.910643     -20.374965   \n",
       "1752349     50     -3.068643     528.790329   -15.910643     -20.374965   \n",
       "\n",
       "         Shortwave_Wm-2  lightExtinct_m-1  ShearVelocity_mS-1  \\\n",
       "0                   0.0               0.4              -999.0   \n",
       "1                   0.0               0.4              -999.0   \n",
       "2                   0.0               0.4              -999.0   \n",
       "3                   0.0               0.4              -999.0   \n",
       "4                   0.0               0.4              -999.0   \n",
       "...                 ...               ...                 ...   \n",
       "1752345             0.0               0.4              -999.0   \n",
       "1752346             0.0               0.4              -999.0   \n",
       "1752347             0.0               0.4              -999.0   \n",
       "1752348             0.0               0.4              -999.0   \n",
       "1752349             0.0               0.4              -999.0   \n",
       "\n",
       "         ShearStress_Nm-2     Area_m2  ...  day_of_year  time_of_day  \\\n",
       "0                  -999.0  39850000.0  ...          364            1   \n",
       "1                  -999.0  39850000.0  ...          364            1   \n",
       "2                  -999.0  39850000.0  ...          364            1   \n",
       "3                  -999.0  39850000.0  ...          364            1   \n",
       "4                  -999.0  39850000.0  ...          364            1   \n",
       "...                   ...         ...  ...          ...          ...   \n",
       "1752345            -999.0  39850000.0  ...          362           23   \n",
       "1752346            -999.0  39850000.0  ...          362           23   \n",
       "1752347            -999.0  39850000.0  ...          362           23   \n",
       "1752348            -999.0  39850000.0  ...          362           23   \n",
       "1752349            -999.0  39850000.0  ...          362           23   \n",
       "\n",
       "         temp_mix03  temp_conv04  temp_initial00  obs_temp  input_obs  \\\n",
       "0          4.089224     4.089224        4.289738  2.532738   2.535714   \n",
       "1          4.205152     4.205152        4.216074  2.707606   2.710317   \n",
       "2          4.312758     4.312758        4.351181  2.707606   2.710317   \n",
       "3          4.432478     4.432478        4.466520  2.707606   2.710317   \n",
       "4          4.497695     4.497695        4.499182  2.707606   2.710317   \n",
       "...             ...          ...             ...       ...        ...   \n",
       "1752345    4.243972     4.243972        4.243906  3.029762   3.030952   \n",
       "1752346    4.337872     4.337872        4.337785  3.185105   3.189226   \n",
       "1752347    4.433367     4.433367        4.433241  4.005785   4.009906   \n",
       "1752348    4.522485     4.522485        4.522366  4.826464   4.830586   \n",
       "1752349    4.619772     4.619772        4.619600  7.996234   7.998070   \n",
       "\n",
       "              ice      snow   snowice  \n",
       "0        0.000000  0.000000  0.000000  \n",
       "1        0.000000  0.000000  0.000000  \n",
       "2        0.000000  0.000000  0.000000  \n",
       "3        0.000000  0.000000  0.000000  \n",
       "4        0.000000  0.000000  0.000000  \n",
       "...           ...       ...       ...  \n",
       "1752345  0.287722  0.025602  0.044282  \n",
       "1752346  0.287722  0.025602  0.044282  \n",
       "1752347  0.287722  0.025602  0.044282  \n",
       "1752348  0.287722  0.025602  0.044282  \n",
       "1752349  0.287722  0.025602  0.044282  \n",
       "\n",
       "[1752350 rows x 45 columns]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"./../02_training/all_data_lake_modeling_in_time.csv\")\n",
    "time = data_df['time']\n",
    "data_df = data_df.drop(columns=['time'])\n",
    "data_df\n",
    "display(data_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days total: 35047\n",
      "Number of training points: 1051400\n"
     ]
    }
   ],
   "source": [
    "training_frac = 0.60\n",
    "depth_steps = 50\n",
    "number_days = len(data_df)//depth_steps\n",
    "n_obs = int(number_days*training_frac)*depth_steps\n",
    "print(f\"Number of days total: {number_days}\")\n",
    "print(f\"Number of training points: {n_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_df.values\n",
    "\n",
    "train_data = data[:n_obs]\n",
    "test_data = data[n_obs:]\n",
    "\n",
    "train_time = time[:n_obs]\n",
    "test_time = time[n_obs:]\n",
    "\n",
    "#performing normalization on all the columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in ['temp_heat01']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Network with Xavier Initialization..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m m0_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(m0_PATH, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     14\u001b[0m heating_model\u001b[38;5;241m.\u001b[39mload_state_dict(m0_checkpoint)\n\u001b[1;32m---> 15\u001b[0m heating_model \u001b[38;5;241m=\u001b[39m \u001b[43mheating_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "m0_input_columns = ['depth', 'AirTemp_degC', 'Longwave_Wm-2', 'Latent_Wm-2', 'Sensible_Wm-2', 'Shortwave_Wm-2',\n",
    "                'lightExtinct_m-1','Area_m2', \n",
    "                 'day_of_year', 'time_of_day', 'ice', 'snow', 'snowice', 'temp_initial00']\n",
    "m0_output_columns = ['temp_heat01']\n",
    "\n",
    "m0_input_column_ix = [data_df.columns.get_loc(column) for column in m0_input_columns]\n",
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in m0_output_columns]\n",
    "\n",
    "m0_PATH =  f\"./../02_training/saved_models/heating_model_time.pth\"\n",
    "m0_layers = [len(m0_input_columns), 32, 32, len(m0_output_columns)]\n",
    "\n",
    "heating_model = MLP(m0_layers, activation=\"gelu\")\n",
    "m0_checkpoint = torch.load(m0_PATH, map_location=torch.device('cpu'))\n",
    "heating_model.load_state_dict(m0_checkpoint)\n",
    "heating_model = heating_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1_input_columns = ['depth', 'AirTemp_degC', 'Longwave_Wm-2', 'Latent_Wm-2', 'Sensible_Wm-2', 'Shortwave_Wm-2',\n",
    "                'lightExtinct_m-1', 'Area_m2', \n",
    "                 'buoyancy', 'day_of_year', 'time_of_day', 'diffusivity', 'temp_heat01']\n",
    "m1_output_columns = ['temp_diff02']\n",
    "\n",
    "m1_input_column_ix = [data_df.columns.get_loc(column) for column in m1_input_columns]\n",
    "m1_output_column_ix = [data_df.columns.get_loc(column) for column in m1_output_columns]\n",
    "\n",
    "m1_PATH = f\"./../02_training/saved_models/diffusion_model_time.pth\"\n",
    "m1_layers = [len(m1_input_columns), 32, 32, len(m1_output_columns)]\n",
    "\n",
    "heat_diff_model = MLP(m1_layers, activation=\"gelu\")\n",
    "m1_checkpoint = torch.load(m1_PATH, map_location=torch.device('cpu'))\n",
    "heat_diff_model.load_state_dict(m1_checkpoint)\n",
    "heat_diff_model = heat_diff_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_input_columns = ['depth', 'ShearVelocity_mS-1', 'ShearStress_Nm-2', 'day_of_year', 'time_of_day',\n",
    "                   'ice', 'snow', 'snowice', 'temp_diff02']\n",
    "m2_output_columns = ['temp_mix03']\n",
    "\n",
    "m2_input_column_ix = [data_df.columns.get_loc(column) for column in m2_input_columns]\n",
    "m2_output_column_ix = [data_df.columns.get_loc(column) for column in m2_output_columns]\n",
    "\n",
    "m2_PATH = f\"./../02_training/saved_models/mixing_model_time.pth\"\n",
    "m2_layers = [len(m2_input_columns), 32, 32, len(m2_output_columns)]\n",
    "\n",
    "mixing_model = MLP(m2_layers, activation=\"gelu\")\n",
    "m2_checkpoint = torch.load(m2_PATH, map_location=torch.device('cpu'))\n",
    "mixing_model.load_state_dict(m2_checkpoint)\n",
    "mixing_model = mixing_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_input_columns = ['depth', 'day_of_year', 'time_of_day', 'temp_diff02']\n",
    "m3_output_columns = ['temp_conv04']\n",
    "\n",
    "m3_input_column_ix = [data_df.columns.get_loc(column) for column in m3_input_columns]\n",
    "m3_output_column_ix = [data_df.columns.get_loc(column) for column in m3_output_columns]\n",
    "\n",
    "m3_PATH = f\"./../02_training/saved_models/convection_model_time.pth\"\n",
    "m3_layers = [len(m3_input_columns), 32, 32, len(m3_output_columns)]\n",
    "\n",
    "convection_model = MLP(m3_layers, activation=\"gelu\")\n",
    "m3_checkpoint = torch.load(m3_PATH, map_location=torch.device('cpu'))\n",
    "convection_model.load_state_dict(m3_checkpoint)\n",
    "convection_model = convection_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_input_columns = ['depth', 'day_of_year', 'time_of_day', 'ice', 'snow', 'snowice',\n",
    "                   'temp_conv04']\n",
    "m4_output_columns = ['temp_total05']\n",
    "\n",
    "m4_input_column_ix = [data_df.columns.get_loc(column) for column in m4_input_columns]\n",
    "m4_output_column_ix = [data_df.columns.get_loc(column) for column in m4_output_columns]\n",
    "\n",
    "m4_PATH = f\"./../02_training/saved_models/ice_model_time.pth\"\n",
    "m4_layers = [len(m4_input_columns), 32, 32, len(m4_output_columns)]\n",
    "\n",
    "ice_model = MLP(m4_layers, activation=\"gelu\")\n",
    "m4_checkpoint = torch.load(m4_PATH, map_location=torch.device('cpu'))\n",
    "ice_model.load_state_dict(m4_checkpoint)\n",
    "ice_model = ice_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_temp_columns = ['obs_temp']\n",
    "\n",
    "obs_temp_columns_ix = [data_df.columns.get_loc(column) for column in obs_temp_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_steps = 24\n",
    "# train_data = np.reshape(train_data, (train_data.shape[0]//depth_steps, depth_steps, train_data.shape[1]))\n",
    "# test_data = np.reshape(test_data, (test_data.shape[0]//depth_steps, depth_steps, test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping track of the mean and standard deviations\n",
    "train_mean = scaler.mean_\n",
    "train_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "batch_size = 1000\n",
    "\n",
    "#assert batch_size % 25 ==0, \"Batchsize has to be multiple of 25\" \n",
    "\n",
    "train_dataset = DataGenerator(train_data)\n",
    "test_dataset = DataGenerator(test_data)\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    return (((true-pred)**2).mean()**0.5).detach().cpu().numpy()\n",
    "\n",
    "def l2_error(true, pred):\n",
    "    return np.linalg.norm(pred.detach().cpu().numpy() - true.detach().cpu().numpy()) / np.linalg.norm(true.detach().cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, loader, input_columns, output_columns, train_mean, train_std):\n",
    "    model.eval()\n",
    "    y_ = []\n",
    "    pred_ = []\n",
    "    mean = torch.tensor(train_mean[output_columns]).to(device)\n",
    "    std = torch.tensor(train_std[output_columns]).to(device)\n",
    "    \n",
    "    for x in iter(loader):\n",
    "        inputs, target = x[:, input_columns].to(device).float(), x[:, output_columns].to(device).float()\n",
    "        pred = model(inputs)\n",
    "        target = target * std + mean\n",
    "        pred = pred * std + mean\n",
    "        y_.append(target)\n",
    "        pred_.append(pred)\n",
    "    y_ = torch.cat(y_, dim=0) \n",
    "    pred_ = torch.cat(pred_, dim=0)\n",
    "    \n",
    "    if y_.shape[1]==2:\n",
    "        rmse_temp = rmse(y_[:,1], pred_[:,1])\n",
    "        l2_error_temp = l2_error(y_[:,1], pred_[:,1])\n",
    "    else:\n",
    "        rmse_temp = rmse(y_[:,0], pred_[:,0])\n",
    "        l2_error_temp = l2_error(y_[:,0], pred_[:,0])\n",
    "        \n",
    "    return rmse_temp, l2_error_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, loader, plot = True):    \n",
    "    heating_model.eval()\n",
    "    heat_diff_model.eval()\n",
    "    mixing_model.eval()\n",
    "    convection_model.eval()\n",
    "    ice_model.eval()\n",
    "\n",
    "    mean = torch.tensor(train_mean[m4_output_column_ix]).float().to(device)\n",
    "    std = torch.tensor(train_std[m4_output_column_ix]).float().to(device)\n",
    "\n",
    "    m1_mean = torch.tensor(train_mean[m1_input_column_ix[-1]]).float().to(device)\n",
    "    m1_std = torch.tensor(train_std[m1_input_column_ix[-1]]).float().to(device)\n",
    "\n",
    "#     depthwise_y_pred = []\n",
    "#     depthwise_y_true = []\n",
    "    y_ = []\n",
    "    y_obs_ = []\n",
    "    pred_ = []\n",
    "        \n",
    "    rmse_models = np.zeros((len(loader), 5))\n",
    "    for ix, x in enumerate(iter(loader)):\n",
    "        x = x.to(device).float()\n",
    "        \n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "            \n",
    "        #model 0\n",
    "        m0_pred = heating_model(m0_input) #predicts diff and temp\n",
    "            \n",
    "        if plot:\n",
    "            m0_y_true = x[:, m0_output_column_ix[0]] * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "            m0_y_pred = m0_pred * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "#             print(m1_y_true.shape, m1_y_pred.shape)\n",
    "#             print(\"True\",m1_y_true)\n",
    "#             print(\"Pred\",m1_y_pred)\n",
    "            rmse_models[ix, 0] = rmse(m0_y_true.squeeze(), m0_y_pred.squeeze())\n",
    "#             print(x[:, m1_output_column_ix[1]])\n",
    "#             print(m1_pred_temp)\n",
    "#             print(criterion(m1_pred_temp, x[:, m1_output_column_ix[1]]))\n",
    "            print(\"RMSE of after m0\", rmse(m0_y_true.flatten(), m0_y_pred.flatten()))\n",
    "\n",
    "        #model 2\n",
    "        m1_input = torch.cat([x[:, m1_input_column_ix[:-1]], m0_pred], dim=-1)\n",
    "\n",
    "        m1_pred = heat_diff_model(m1_input)\n",
    "\n",
    "        \n",
    "        #print(m1_pred)\n",
    "        \n",
    "        if plot:\n",
    "            m1_y_true = x[:, m1_output_column_ix[0]] * torch.tensor(train_std[m1_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m1_output_column_ix[0]]).to(device)\n",
    "            m1_y_pred = m1_pred * torch.tensor(train_std[m1_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m1_output_column_ix[0]]).to(device)\n",
    "#             print(m1_y_true.shape, m1_y_pred.shape)\n",
    "#             print(\"True\",m1_y_true)\n",
    "#             print(\"Pred\",m1_y_pred)\n",
    "            rmse_models[ix, 1] = rmse(m1_y_true.squeeze(), m1_y_pred.squeeze())\n",
    "#             print(x[:, m1_output_column_ix[1]])\n",
    "#             print(m1_pred_temp)\n",
    "#             print(criterion(m1_pred_temp, x[:, m1_output_column_ix[1]]))\n",
    "            print(\"RMSE of after m1\", rmse(m1_y_true.flatten(), m1_y_pred.flatten()))\n",
    "\n",
    "\n",
    "        #model 3\n",
    "        m3_input = torch.cat([x[:, m3_input_column_ix[:-1]], m1_pred], dim=-1)\n",
    "        m3_pred = convection_model(m3_input)\n",
    "            \n",
    "        if plot:\n",
    "            m3_y_true = x[:, m3_output_column_ix] * torch.tensor(train_std[m3_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m3_output_column_ix[0]]).to(device)\n",
    "            m3_y_pred = m3_pred * torch.tensor(train_std[m3_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m3_output_column_ix[0]]).to(device)\n",
    "            rmse_models[ix, 3] = rmse(m3_y_true.squeeze(), m3_y_pred.squeeze())\n",
    "            print(\"RMSE of after m3\", rmse(m3_y_true.flatten(), m3_y_pred.flatten()))\n",
    "\n",
    "        #model 4\n",
    "        m4_input = torch.cat([x[:, m4_input_column_ix[:-1]], m3_pred], dim=-1)\n",
    "        m4_pred = ice_model(m4_input)\n",
    "            \n",
    "        if plot:\n",
    "            m4_y_true = x[:, m4_output_column_ix] * std + mean\n",
    "            m4_y_pred = m4_pred * std + mean\n",
    "            rmse_models[ix, 4] = rmse(m4_y_true.squeeze(), m4_y_pred.squeeze())\n",
    "            print(\"RMSE of after m4\", rmse(m4_y_true.flatten(), m4_y_pred.flatten()))\n",
    "\n",
    "        y_true = x[:, m4_output_column_ix] * std + mean\n",
    "        y_obs = x[:, obs_temp_columns_ix] * train_std[obs_temp_columns_ix[0]] + train_mean[obs_temp_columns_ix[0]]\n",
    "        pred = m4_pred * std + mean\n",
    "        \n",
    "        y_.append(y_true)\n",
    "        y_obs_.append(y_obs)\n",
    "        pred_.append(pred)\n",
    "\n",
    "    y_ = torch.cat(y_, dim=0)\n",
    "    y_obs_ = torch.cat(y_obs_, dim=0)\n",
    "    pred_ = torch.cat(pred_, dim=0) \n",
    "    \n",
    "    #if plot:   \n",
    "     #        rmse_models = rmse_models.mean(axis=0)\n",
    "     #        plt.figure(figsize=(12,8))\n",
    "     #        plt.plot(rmse_models[:, 0], label=\"RMSE after Heating Model\")\n",
    "     #        plt.plot(rmse_models[:, 1], label=\"RMSE after Heat-Diffusion Model\")\n",
    "     #        plt.plot(rmse_models[:, 2], label=\"RMSE after Mixing Model\")\n",
    "     #        plt.plot(rmse_models[:, 3], label=\"RMSE after Convection Model\")\n",
    "     #        plt.plot(rmse_models[:, 4], label=\"RMSE after Ice Model\")\n",
    "     #        plt.legend(loc=\"upper left\", fontsize=12)\n",
    "     #        plt.xlabel(\"Depth\", fontsize=12)\n",
    "     #        plt.ylabel(\"RMSE\", fontsize=12)\n",
    "     #        plt.grid(\"on\", alpha=0.5)\n",
    "     #        plt.show()\n",
    "    \n",
    "    return pred_, y_, y_obs_, rmse_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y_pred, y_true, depth_steps, time_label, figsize=(20,10)):\n",
    "    time_label = np.array([time[:10] for time in time_label])\n",
    "    time_label = time_label[::depth_steps]\n",
    "    \n",
    "    y_pred = y_pred.flatten().detach().cpu().numpy()\n",
    "    y_true = y_true.flatten().detach().cpu().numpy()\n",
    "    \n",
    "    y_true = np.reshape(y_true, (y_true.shape[0]//depth_steps, depth_steps))\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0]//depth_steps, depth_steps))\n",
    "    \n",
    "    N_pts = 6 # number of points to display on the x-label\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1, figsize=figsize)\n",
    "    sns.heatmap(y_true.T, ax=ax[0], cmap='seismic', vmin=0., vmax=35.)\n",
    "    ax[0].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[0].set_xlabel(\"Time\", fontsize=15)\n",
    "    \n",
    "    xticks_ix = np.array(ax[0].get_xticks()).astype(int)\n",
    "    time_label = time_label[xticks_ix]\n",
    "    nelement = len(time_label)//N_pts\n",
    "    time_label = time_label[::nelement]\n",
    "    ax[0].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[0].set_xticklabels(time_label, rotation=0)   \n",
    "#     ax[0].xaxis.set_major_locator(plt.MultipleLocator(100))\n",
    "    ax[0].collections[0].colorbar.set_label(\"Actual Temperature\")\n",
    "    \n",
    "    sns.heatmap(y_pred.T, ax=ax[1], cmap='seismic', vmin=0., vmax=35.)\n",
    "    ax[1].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[1].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[1].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[1].set_xticklabels(time_label, rotation=0)\n",
    "    ax[1].collections[0].colorbar.set_label(\"Predicted Temperature\")\n",
    "    \n",
    "    sns.heatmap(np.abs(y_pred.T-y_true.T), ax=ax[2], cmap='viridis')\n",
    "    ax[2].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[2].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[2].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[2].set_xticklabels(time_label, rotation=0)\n",
    "    \n",
    "    ax[2].collections[0].colorbar.set_label(\"Absolute Error\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(heating_model,heat_diff_model, mixing_model, convection_model, ice_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics( heat_diff_model, train_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(heat_diff_model, test_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(convection_model, train_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(convection_model, test_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(ice_model, train_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(ice_model, test_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "# decay_rate = 0.1\n",
    "# decay_steps = 500\n",
    "\n",
    "params = list(heating_model.parameters()) + list(heat_diff_model.parameters()) + list(convection_model.parameters()) + list(ice_model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=lr, \n",
    "                             betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=decay_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze_model(heating_model)\n",
    "freeze_model(heating_model)\n",
    "unfreeze_model(heat_diff_model)\n",
    "freeze_model(convection_model)\n",
    "freeze_model(ice_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on Observed Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating_model.train()\n",
    "heat_diff_model.train()\n",
    "convection_model.train()\n",
    "ice_model.train()\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "# mean and standard dev of \"temp_total04\" of model 4 output\n",
    "mean_out4 = torch.tensor(train_mean[m4_output_column_ix[0]]).float().to(device)\n",
    "std_out4 = torch.tensor(train_std[m4_output_column_ix[0]]).float().to(device)\n",
    "\n",
    "mean_obs = torch.tensor(train_mean[obs_temp_columns_ix[0]]).float().to(device)\n",
    "std_obs = torch.tensor(train_std[obs_temp_columns_ix[0]]).float().to(device)\n",
    "\n",
    "# mean and standard dev of \"input_temp\" of model 1 input\n",
    "m1_mean = torch.tensor(train_mean[m1_input_column_ix[-1]]).float().to(device)\n",
    "m1_std = torch.tensor(train_std[m1_input_column_ix[-1]]).float().to(device)\n",
    "\n",
    "train_loss = []\n",
    "LOSS_m0 = []\n",
    "LOSS_m1 = []\n",
    "LOSS_m2 = []\n",
    "LOSS_m3 = []\n",
    "LOSS_m4 = []\n",
    "\n",
    "for it in tqdm(range(n_epochs)):\n",
    "    loss_epoch = 0\n",
    "    loss_epoch_m0 = 0\n",
    "    loss_epoch_m1 = 0\n",
    "    loss_epoch_m2 = 0\n",
    "    loss_epoch_m3 = 0\n",
    "    loss_epoch_m4 = 0\n",
    "    for ix, x in enumerate(iter(train_loader)):\n",
    "        x = x.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "      #  m1_input = x[:, m1_input_column_ix]\n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "        \n",
    "        #model 0\n",
    "        m0_pred = heating_model(m0_input) #predicts diff and temp\n",
    "        loss_m0 = criterion(m0_pred, x[:, m0_output_column_ix])\n",
    "\n",
    "        #model 1\n",
    "        m1_input = torch.cat([x[:, m1_input_column_ix[:-1]], m0_pred], dim=-1)\n",
    "        # m1_pred = heat_diff_model(m1_input)\n",
    "        \n",
    "        m1_pred = heat_diff_model(m1_input)\n",
    "        loss_m1 = criterion(m1_pred, x[:, m1_output_column_ix])\n",
    "        \n",
    "        #m1_pred = heat_diff_model(m1_input) #predicts diff and temp\n",
    "        #m1_pred_temp = m1_pred[:,1:2]\n",
    "            \n",
    "        #loss_m1 = criterion(m1_pred_temp, x[:, m1_output_column_ix[1]].unsqueeze(1))\n",
    "\n",
    "        #model 3\n",
    "        m3_input = torch.cat([x[:, m3_input_column_ix[:-1]], m1_pred], dim=-1)\n",
    "        m3_pred = convection_model(m3_input)\n",
    "\n",
    "        loss_m3 = criterion(m3_pred, x[:, m3_output_column_ix])\n",
    "\n",
    "        #model 4\n",
    "        m4_input = torch.cat([x[:, m4_input_column_ix[:-1]], m3_pred], dim=-1)\n",
    "        m4_pred = ice_model(m4_input)\n",
    "        \n",
    "        obs_temp_true = x[:, obs_temp_columns_ix] * std_obs + mean_obs\n",
    "        obs_temp_true_norm = (obs_temp_true - mean_out4)/std_out4\n",
    "        \n",
    "        loss_m4 = criterion(m4_pred, obs_temp_true_norm)\n",
    "\n",
    "        #loss = (loss_m0 + loss_m1 + loss_m2 + loss_m3 + loss_m4)\n",
    "        \n",
    "        loss = loss_m4\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss.item()\n",
    "        loss_epoch_m0 += loss_m0.item()\n",
    "        loss_epoch_m1 += loss_m1.item()\n",
    "        loss_epoch_m3 += loss_m3.item()\n",
    "        loss_epoch_m4 += loss_m4.item()\n",
    "    \n",
    "    loss_epoch = loss_epoch/len(train_loader)\n",
    "    loss_epoch_m0 = loss_epoch_m0/len(train_loader)\n",
    "    loss_epoch_m1 = loss_epoch_m1/len(train_loader)\n",
    "    loss_epoch_m3 = loss_epoch_m3/len(train_loader)\n",
    "    loss_epoch_m4 = loss_epoch_m4/len(train_loader)\n",
    "    \n",
    "    train_loss.append(loss_epoch)\n",
    "    LOSS_m0.append(loss_epoch_m0)\n",
    "    LOSS_m1.append(loss_epoch_m1)\n",
    "    LOSS_m3.append(loss_epoch_m3)\n",
    "    LOSS_m4.append(loss_epoch_m4)\n",
    "    if it % 50 == 0:\n",
    "        print(f\"Epoch : {it}, Train_loss: {train_loss[-1]}, Loss m0: {LOSS_m0[-1]}, Loss m1: {LOSS_m1[-1]},  Loss m3: {LOSS_m3[-1]}, Loss m4: {LOSS_m4[-1]}\")\n",
    "    \n",
    "    #plot the loss_m1, m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss, label = \"Train Loss\")\n",
    "plt.plot(LOSS_m0, label = \"Loss M0\")\n",
    "plt.plot(LOSS_m1, label = \"Loss M1\")\n",
    "#plt.plot(LOSS_m2, label = \"Loss M2\")\n",
    "plt.plot(LOSS_m3, label = \"Loss M3\")\n",
    "plt.plot(LOSS_m4, label = \"Loss M4\")\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=15)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Evaluation After FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(heat_diff_model, train_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(heat_diff_model, test_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(convection_model, train_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(convection_model, test_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(ice_model, train_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(ice_model, test_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Model Similarity after Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_similarity(model1, model2):\n",
    "    weight1 = []\n",
    "    weight2 = []\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        weight1.append(param1.detach().clone().flatten())\n",
    "        weight2.append(param2.detach().clone().flatten())\n",
    "    weight1 = torch.cat(weight1, dim=0)\n",
    "    weight2 = torch.cat(weight2, dim=0)\n",
    "    \n",
    "    #Cosine Similarity\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
    "    cos_sim = cos(weight1, weight2)\n",
    "    \n",
    "    #L2 norm\n",
    "    l2 = torch.norm((weight1-weight2), p='fro', dim=0)\n",
    "    \n",
    "    return cos_sim, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating_model_init = MLP(m0_layers, activation=\"gelu\")\n",
    "heating_model_init.load_state_dict(m0_checkpoint)\n",
    "heating_model_init = heating_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(heating_model_init, heating_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_diff_model_init = MLP(m1_layers, activation=\"gelu\")\n",
    "heat_diff_model_init.load_state_dict(m1_checkpoint)\n",
    "heat_diff_model_init = heat_diff_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(heat_diff_model_init, heat_diff_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convection_model_init = MLP(m3_layers, activation=\"gelu\")\n",
    "convection_model_init.load_state_dict(m3_checkpoint)\n",
    "convection_model_init = convection_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(convection_model_init, convection_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_model_init = MLP(m4_layers, activation=\"gelu\")\n",
    "ice_model_init.load_state_dict(m4_checkpoint)\n",
    "ice_model_init = ice_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(ice_model_init, ice_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all models again and compute finetuned diffusivity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze diffusion model and finetune it on projected diffusivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
