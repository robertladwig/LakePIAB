{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does not make sense\n",
    "\n",
    "Cannot be finetuned with all 4 losses. The term 1 and term 4 are contradictory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ladwi\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layers, activation=\"relu\", init=\"xavier\"):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = torch.nn.Tanh()\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"Unspecified activation type\")\n",
    "        \n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "        if init==\"xavier\":\n",
    "            self.xavier_init_weights()\n",
    "        elif init==\"kaiming\":\n",
    "            self.kaiming_init_weights()\n",
    "    \n",
    "    def xavier_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Xavier Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def kaiming_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Kaiming Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.kaiming_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "    \n",
    "class DataGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          depth  AirTemp_degC  Longwave_Wm-2  Latent_Wm-2  Sensible_Wm-2  \\\n",
       "0            1     -2.989824     551.514698   -19.194445     -28.116538   \n",
       "1            2     -2.989824     551.514698   -19.194445     -28.116538   \n",
       "2            3     -2.989824     551.514698   -19.194445     -28.116538   \n",
       "3            4     -2.989824     551.514698   -19.194445     -28.116538   \n",
       "4            5     -2.989824     551.514698   -19.194445     -28.116538   \n",
       "...        ...           ...            ...          ...            ...   \n",
       "2628545     46    -12.920028     494.226632   -46.982710     -95.220702   \n",
       "2628546     47    -12.920028     494.226632   -46.982710     -95.220702   \n",
       "2628547     48    -12.920028     494.226632   -46.982710     -95.220702   \n",
       "2628548     49    -12.920028     494.226632   -46.982710     -95.220702   \n",
       "2628549     50    -12.920028     494.226632   -46.982710     -95.220702   \n",
       "\n",
       "         Shortwave_Wm-2  lightExtinct_m-1  ShearVelocity_mS-1  \\\n",
       "0                   0.0               0.4              -999.0   \n",
       "1                   0.0               0.4              -999.0   \n",
       "2                   0.0               0.4              -999.0   \n",
       "3                   0.0               0.4              -999.0   \n",
       "4                   0.0               0.4              -999.0   \n",
       "...                 ...               ...                 ...   \n",
       "2628545             0.0               0.4              -999.0   \n",
       "2628546             0.0               0.4              -999.0   \n",
       "2628547             0.0               0.4              -999.0   \n",
       "2628548             0.0               0.4              -999.0   \n",
       "2628549             0.0               0.4              -999.0   \n",
       "\n",
       "         ShearStress_Nm-2     Area_m2  ...  day_of_year  time_of_day  \\\n",
       "0                  -999.0  39850000.0  ...          365            1   \n",
       "1                  -999.0  39850000.0  ...          365            1   \n",
       "2                  -999.0  39850000.0  ...          365            1   \n",
       "3                  -999.0  39850000.0  ...          365            1   \n",
       "4                  -999.0  39850000.0  ...          365            1   \n",
       "...                   ...         ...  ...          ...          ...   \n",
       "2628545            -999.0  39850000.0  ...          362           23   \n",
       "2628546            -999.0  39850000.0  ...          362           23   \n",
       "2628547            -999.0  39850000.0  ...          362           23   \n",
       "2628548            -999.0  39850000.0  ...          362           23   \n",
       "2628549            -999.0  39850000.0  ...          362           23   \n",
       "\n",
       "         temp_mix03  temp_conv04  temp_initial00  obs_temp  input_obs  \\\n",
       "0          1.189659     1.189659        1.400563  5.773598   5.776641   \n",
       "1          1.337920     1.337920        1.400563  5.978895   5.981724   \n",
       "2          1.719089     1.719089        1.378568  6.204667   6.207320   \n",
       "3          2.013660     2.013660        1.443240  6.260337   6.263034   \n",
       "4          2.087996     2.087996        1.729038  6.319616   6.322359   \n",
       "...             ...          ...             ...       ...        ...   \n",
       "2628545    4.066941     4.066941        4.066863  3.029762   3.030952   \n",
       "2628546    4.172542     4.172542        4.172443  3.185105   3.189226   \n",
       "2628547    4.282669     4.282669        4.282541  4.005785   4.009906   \n",
       "2628548    4.386953     4.386953        4.386816  4.826464   4.830586   \n",
       "2628549    4.503658     4.503658        4.503485  7.996234   7.998070   \n",
       "\n",
       "             ice     snow  snowice  \n",
       "0        0.00000  0.00000      0.0  \n",
       "1        0.00000  0.00000      0.0  \n",
       "2        0.00000  0.00000      0.0  \n",
       "3        0.00000  0.00000      0.0  \n",
       "4        0.00000  0.00000      0.0  \n",
       "...          ...      ...      ...  \n",
       "2628545  0.47505  0.00527      0.0  \n",
       "2628546  0.47505  0.00527      0.0  \n",
       "2628547  0.47505  0.00527      0.0  \n",
       "2628548  0.47505  0.00527      0.0  \n",
       "2628549  0.47505  0.00527      0.0  \n",
       "\n",
       "[2628550 rows x 45 columns]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"./../02_training/all_data_lake_modeling_in_time.csv\")\n",
    "time = data_df['time']\n",
    "data_df = data_df.drop(columns=['time'])\n",
    "data_df\n",
    "display(data_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days total: 52571\n",
      "Number of training points: 1577100\n"
     ]
    }
   ],
   "source": [
    "training_frac = 0.60\n",
    "depth_steps = 50\n",
    "number_days = len(data_df)//depth_steps\n",
    "n_obs = int(number_days*training_frac)*depth_steps\n",
    "print(f\"Number of days total: {number_days}\")\n",
    "print(f\"Number of training points: {n_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_df.values\n",
    "\n",
    "train_data = data[:n_obs]\n",
    "test_data = data[n_obs:]\n",
    "\n",
    "train_time = time[:n_obs]\n",
    "test_time = time[n_obs:]\n",
    "\n",
    "#performing normalization on all the columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in ['temp_conv04']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Network with Xavier Initialization..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MLP:\n\tsize mismatch for layers.layer_0.weight: copying a param with shape torch.Size([32, 8]) from checkpoint, the shape in current model is torch.Size([32, 9]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m direct_model \u001b[38;5;241m=\u001b[39m MLP(m0_layers, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m m0_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(m0_PATH, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 17\u001b[0m \u001b[43mdirect_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm0_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m direct_model \u001b[38;5;241m=\u001b[39m direct_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MLP:\n\tsize mismatch for layers.layer_0.weight: copying a param with shape torch.Size([32, 8]) from checkpoint, the shape in current model is torch.Size([32, 9])."
     ]
    }
   ],
   "source": [
    "m0_input_columns = ['depth', 'AirTemp_degC', 'Shortwave_Wm-2',\n",
    "                'lightExtinct_m-1','Area_m2', 'Uw',\n",
    "                 'day_of_year', 'time_of_day', 'temp_initial00'] #,  \n",
    "               #  'buoyancy', 'diffusivity', 'temp_initial00', \n",
    "               # 'temp_heat01', 'temp_diff02', 'temp_total05',\n",
    "               # 'ice', 'snow', 'snowice'\n",
    "m0_output_columns =['temp_conv04']\n",
    "\n",
    "m0_input_column_ix = [data_df.columns.get_loc(column) for column in m0_input_columns]\n",
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in m0_output_columns]\n",
    "\n",
    "m0_PATH =  f\"./../02_training/saved_models/direct_model_train_pb_time.pth\"\n",
    "m0_layers = [len(m0_input_columns), 32, 32,32,32,32,32,32,32,32,32, len(m0_output_columns)]\n",
    "\n",
    "direct_model = MLP(m0_layers, activation=\"gelu\")\n",
    "m0_checkpoint = torch.load(m0_PATH, map_location=torch.device('cpu'))\n",
    "direct_model.load_state_dict(m0_checkpoint)\n",
    "direct_model = direct_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_temp_columns = ['obs_temp']\n",
    "\n",
    "obs_temp_columns_ix = [data_df.columns.get_loc(column) for column in obs_temp_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_steps = 24\n",
    "# train_data = np.reshape(train_data, (train_data.shape[0]//depth_steps, depth_steps, train_data.shape[1]))\n",
    "# test_data = np.reshape(test_data, (test_data.shape[0]//depth_steps, depth_steps, test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping track of the mean and standard deviations\n",
    "train_mean = scaler.mean_\n",
    "train_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "batch_size = 1000\n",
    "\n",
    "#assert batch_size % 25 ==0, \"Batchsize has to be multiple of 25\" \n",
    "\n",
    "train_dataset = DataGenerator(train_data)\n",
    "test_dataset = DataGenerator(test_data)\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    return (((true-pred)**2).mean()**0.5).detach().cpu().numpy()\n",
    "\n",
    "def l2_error(true, pred):\n",
    "    return np.linalg.norm(pred.detach().cpu().numpy() - true.detach().cpu().numpy()) / np.linalg.norm(true.detach().cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, loader, input_columns, output_columns, train_mean, train_std):\n",
    "    model.eval()\n",
    "    y_ = []\n",
    "    pred_ = []\n",
    "    mean = torch.tensor(train_mean[output_columns]).to(device)\n",
    "    std = torch.tensor(train_std[output_columns]).to(device)\n",
    "    \n",
    "    for x in iter(loader):\n",
    "        inputs, target = x[:, input_columns].to(device).float(), x[:, output_columns].to(device).float()\n",
    "        pred = model(inputs)\n",
    "        target = target * std + mean\n",
    "        pred = pred * std + mean\n",
    "        y_.append(target)\n",
    "        pred_.append(pred)\n",
    "    y_ = torch.cat(y_, dim=0) \n",
    "    pred_ = torch.cat(pred_, dim=0)\n",
    "    \n",
    "    if y_.shape[1]==2:\n",
    "        rmse_temp = rmse(y_[:,1], pred_[:,1])\n",
    "        l2_error_temp = l2_error(y_[:,1], pred_[:,1])\n",
    "    else:\n",
    "        rmse_temp = rmse(y_[:,0], pred_[:,0])\n",
    "        l2_error_temp = l2_error(y_[:,0], pred_[:,0])\n",
    "        \n",
    "    return rmse_temp, l2_error_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollout_predictions(direct_model, loader, plot = True):    \n",
    "    direct_model.eval()\n",
    "\n",
    "    mean = torch.tensor(train_mean[m0_output_column_ix]).float().to(device)\n",
    "    std = torch.tensor(train_std[m0_output_column_ix]).float().to(device)\n",
    "\n",
    "\n",
    "#     depthwise_y_pred = []\n",
    "#     depthwise_y_true = []\n",
    "    y_ = []\n",
    "    y_obs_ = []\n",
    "    pred_ = []\n",
    "        \n",
    "    rmse_models = np.zeros((len(loader), 5))\n",
    "    for ix, x in enumerate(iter(loader)):\n",
    "        x = x.to(device).float()\n",
    "        \n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "            \n",
    "        #model 0\n",
    "        m0_pred = direct_model(m0_input) #predicts diff and temp\n",
    "            \n",
    "        if plot:\n",
    "            m0_y_true = x[:, m0_output_column_ix[0]] * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "            m0_y_pred = m0_pred * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "#             print(m1_y_true.shape, m1_y_pred.shape)\n",
    "#             print(\"True\",m1_y_true)\n",
    "#             print(\"Pred\",m1_y_pred)\n",
    "            rmse_models[ix, 0] = rmse(m0_y_true.squeeze(), m0_y_pred.squeeze())\n",
    "#             print(x[:, m1_output_column_ix[1]])\n",
    "#             print(m1_pred_temp)\n",
    "#             print(criterion(m1_pred_temp, x[:, m1_output_column_ix[1]]))\n",
    "            print(\"RMSE of after m0\", rmse(m0_y_true.flatten(), m0_y_pred.flatten()))\n",
    "\n",
    "        y_true = x[:, m0_output_column_ix] * std + mean\n",
    "        y_obs = x[:, obs_temp_columns_ix] * train_std[obs_temp_columns_ix[0]] + train_mean[obs_temp_columns_ix[0]]\n",
    "        pred = m0_pred * std + mean\n",
    "        \n",
    "        y_.append(y_true)\n",
    "        y_obs_.append(y_obs)\n",
    "        pred_.append(pred)\n",
    "\n",
    "    y_ = torch.cat(y_, dim=0)\n",
    "    y_obs_ = torch.cat(y_obs_, dim=0)\n",
    "    pred_ = torch.cat(pred_, dim=0) \n",
    "    \n",
    "    #if plot:   \n",
    "     #        rmse_models = rmse_models.mean(axis=0)\n",
    "     #        plt.figure(figsize=(12,8))\n",
    "     #        plt.plot(rmse_models[:, 0], label=\"RMSE after Heating Model\")\n",
    "     #        plt.plot(rmse_models[:, 1], label=\"RMSE after Heat-Diffusion Model\")\n",
    "     #        plt.plot(rmse_models[:, 2], label=\"RMSE after Mixing Model\")\n",
    "     #        plt.plot(rmse_models[:, 3], label=\"RMSE after Convection Model\")\n",
    "     #        plt.plot(rmse_models[:, 4], label=\"RMSE after Ice Model\")\n",
    "     #        plt.legend(loc=\"upper left\", fontsize=12)\n",
    "     #        plt.xlabel(\"Depth\", fontsize=12)\n",
    "     #        plt.ylabel(\"RMSE\", fontsize=12)\n",
    "     #        plt.grid(\"on\", alpha=0.5)\n",
    "     #        plt.show()\n",
    "    \n",
    "    return pred_, y_, y_obs_, rmse_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y_pred, y_true, depth_steps, time_label, figsize=(20,10)):\n",
    "    time_label = np.array([time[:10] for time in time_label])\n",
    "    time_label = time_label[::depth_steps]\n",
    "    \n",
    "    y_pred = y_pred.flatten().detach().cpu().numpy()\n",
    "    y_true = y_true.flatten().detach().cpu().numpy()\n",
    "    \n",
    "    y_true = np.reshape(y_true, (y_true.shape[0]//depth_steps, depth_steps))\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0]//depth_steps, depth_steps))\n",
    "    \n",
    "    N_pts = 6 # number of points to display on the x-label\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1, figsize=figsize)\n",
    "    sns.heatmap(y_true.T, ax=ax[0], cmap='Spectral_r', vmin=0., vmax=35.)\n",
    "    ax[0].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[0].set_xlabel(\"Time\", fontsize=15)\n",
    "    \n",
    "    xticks_ix = np.array(ax[0].get_xticks()).astype(int)\n",
    "    time_label = time_label[xticks_ix]\n",
    "    nelement = len(time_label)//N_pts\n",
    "    time_label = time_label[::nelement]\n",
    "    ax[0].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[0].set_xticklabels(time_label, rotation=0)   \n",
    "#     ax[0].xaxis.set_major_locator(plt.MultipleLocator(100))\n",
    "    ax[0].collections[0].colorbar.set_label(\"Actual Temperature\")\n",
    "    \n",
    "    sns.heatmap(y_pred.T, ax=ax[1], cmap='Spectral_r', vmin=0., vmax=35.)\n",
    "    ax[1].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[1].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[1].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[1].set_xticklabels(time_label, rotation=0)\n",
    "    ax[1].collections[0].colorbar.set_label(\"Predicted Temperature\")\n",
    "    \n",
    "    sns.heatmap(np.abs(y_pred.T-y_true.T), ax=ax[2], cmap='viridis')\n",
    "    ax[2].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[2].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[2].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[2].set_xticklabels(time_label, rotation=0)\n",
    "    \n",
    "    ax[2].collections[0].colorbar.set_label(\"Absolute Error\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(direct_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(direct_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics( direct_model, train_loader, m0_input_column_ix, m0_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(direct_model, test_loader, m0_input_column_ix, m0_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "# decay_rate = 0.1\n",
    "# decay_steps = 500\n",
    "\n",
    "params = list(direct_model.parameters()) \n",
    "optimizer = torch.optim.Adam(params, lr=lr, \n",
    "                             betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=decay_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze_model(heating_model)\n",
    "\n",
    "unfreeze_model(direct_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on Observed Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_model.train()\n",
    "\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "# mean and standard dev of \"temp_total04\" of model 4 output\n",
    "mean_out0 = torch.tensor(train_mean[m0_output_column_ix[0]]).float().to(device)\n",
    "std_out0 = torch.tensor(train_std[m0_output_column_ix[0]]).float().to(device)\n",
    "\n",
    "mean_obs = torch.tensor(train_mean[obs_temp_columns_ix[0]]).float().to(device)\n",
    "std_obs = torch.tensor(train_std[obs_temp_columns_ix[0]]).float().to(device)\n",
    "\n",
    "# mean and standard dev of \"input_temp\" of model 1 input\n",
    "\n",
    "train_loss = []\n",
    "LOSS_m0 = []\n",
    "\n",
    "\n",
    "for it in tqdm(range(n_epochs)):\n",
    "    loss_epoch = 0\n",
    "    loss_epoch_m0 = 0\n",
    "\n",
    "    for ix, x in enumerate(iter(train_loader)):\n",
    "        x = x.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "      #  m1_input = x[:, m1_input_column_ix]\n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "        \n",
    "        #model 0\n",
    "        m0_pred = direct_model(m0_input) #predicts diff and temp\n",
    "        \n",
    "        obs_temp_true = x[:, obs_temp_columns_ix] * std_obs + mean_obs\n",
    "        obs_temp_true_norm = (obs_temp_true - mean_out0)/std_out0\n",
    "        \n",
    "        loss_m0 = criterion(m0_pred, obs_temp_true_norm)\n",
    "\n",
    "        #loss = (loss_m0 + loss_m1 + loss_m2 + loss_m3 + loss_m4)\n",
    "        \n",
    "        loss = loss_m0\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss.item()\n",
    "        loss_epoch_m0 += loss_m0.item()\n",
    "\n",
    "    loss_epoch = loss_epoch/len(train_loader)\n",
    "    loss_epoch_m0 = loss_epoch_m0/len(train_loader)\n",
    "\n",
    "    train_loss.append(loss_epoch)\n",
    "    LOSS_m0.append(loss_epoch_m0)\n",
    "\n",
    "    if it % 50 == 0:\n",
    "        print(f\"Epoch : {it}, Train_loss: {train_loss[-1]}, Loss m0: {LOSS_m0[-1]}\")\n",
    "    \n",
    "    #plot the loss_m1, m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss, label = \"Train Loss\")\n",
    "plt.plot(LOSS_m0, label = \"Loss M0\")\n",
    "\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=15)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Evaluation After FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(direct_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(direct_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Model Similarity after Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all models again and compute finetuned diffusivity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze diffusion model and finetune it on projected diffusivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"./saved_models/directdeep_model_PB_finetuned.pth\"\n",
    "torch.save(direct_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
