{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does not make sense\n",
    "\n",
    "Cannot be finetuned with all 4 losses. The term 1 and term 4 are contradictory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layers, activation=\"relu\", init=\"xavier\"):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = torch.nn.Tanh()\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"Unspecified activation type\")\n",
    "        \n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "        if init==\"xavier\":\n",
    "            self.xavier_init_weights()\n",
    "        elif init==\"kaiming\":\n",
    "            self.kaiming_init_weights()\n",
    "    \n",
    "    def xavier_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Xavier Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def kaiming_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Kaiming Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.kaiming_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "    \n",
    "class DataGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3272f79c7db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./../02_training/all_data_lake_modeling_in_time.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m     \"\"\"\n\u001b[1;32m   1422\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"./../02_training/all_data_lake_modeling_in_time.csv\")\n",
    "data_df = data_df.fillna('')\n",
    "time = data_df['time']\n",
    "data_df = data_df.drop(columns=['time'])\n",
    "data_df\n",
    "display(data_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frac = 0.60\n",
    "depth_steps = 25\n",
    "number_days = len(data_df)//depth_steps\n",
    "n_obs = int(number_days*training_frac)*depth_steps\n",
    "print(f\"Number of days total: {number_days}\")\n",
    "print(f\"Number of training points: {n_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_df.values\n",
    "\n",
    "train_data = data[:n_obs]\n",
    "test_data = data[n_obs:]\n",
    "\n",
    "train_time = time[:n_obs]\n",
    "test_time = time[n_obs:]\n",
    "\n",
    "#performing normalization on all the columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in ['temp_heat01']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_input_columns = ['depth', 'AirTemp_degC', 'Longwave_Wm-2', 'Latent_Wm-2', 'Sensible_Wm-2', 'Shortwave_Wm-2',\n",
    "                'lightExtinct_m-1','Area_m2', \n",
    "                 'day_of_year', 'time_of_day', 'ice', 'snow', 'snowice','temp_initial00']\n",
    "m0_output_columns = ['temp_heat01']\n",
    "\n",
    "m0_input_column_ix = [data_df.columns.get_loc(column) for column in m0_input_columns]\n",
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in m0_output_columns]\n",
    "\n",
    "m0_PATH =  f\"./../02_training/saved_models/heating_model_time.pth\"\n",
    "m0_layers = [len(m0_input_columns), 32, 32, len(m0_output_columns)]\n",
    "\n",
    "heating_model = MLP(m0_layers, activation=\"gelu\")\n",
    "m0_checkpoint = torch.load(m0_PATH, map_location=torch.device('cpu'))\n",
    "heating_model.load_state_dict(m0_checkpoint)\n",
    "heating_model = heating_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1_input_columns = ['depth', 'AirTemp_degC', 'Longwave_Wm-2', 'Latent_Wm-2', 'Sensible_Wm-2', 'Shortwave_Wm-2',\n",
    "                'lightExtinct_m-1', 'ShearVelocity_mS-1', 'ShearStress_Nm-2', 'Area_m2', \n",
    "                 'buoyancy', 'day_of_year', 'time_of_day', 'diffusivity', 'temp_heat01']\n",
    "m1_output_columns = ['temp_diff02']\n",
    "\n",
    "m1_input_column_ix = [data_df.columns.get_loc(column) for column in m1_input_columns]\n",
    "m1_output_column_ix = [data_df.columns.get_loc(column) for column in m1_output_columns]\n",
    "\n",
    "m1_PATH = f\"./../02_training/saved_models/heat_diffusion_model_time.pth\"\n",
    "m1_layers = [len(m1_input_columns), 32, 32, len(m1_output_columns)]\n",
    "\n",
    "heat_diff_model = MLP(m1_layers, activation=\"gelu\")\n",
    "m1_checkpoint = torch.load(m1_PATH, map_location=torch.device('cpu'))\n",
    "heat_diff_model.load_state_dict(m1_checkpoint)\n",
    "heat_diff_model = heat_diff_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_input_columns = ['depth', 'ShearVelocity_mS-1', 'ShearStress_Nm-2', 'day_of_year', 'time_of_day', \n",
    "                    'ice', 'snow', 'snowice','temp_diff02']\n",
    "m2_output_columns = ['temp_mix03']\n",
    "\n",
    "m2_input_column_ix = [data_df.columns.get_loc(column) for column in m2_input_columns]\n",
    "m2_output_column_ix = [data_df.columns.get_loc(column) for column in m2_output_columns]\n",
    "\n",
    "m2_PATH = f\"./../02_training/saved_models/mixing_model_time.pth\"\n",
    "m2_layers = [len(m2_input_columns), 32, 32, len(m2_output_columns)]\n",
    "\n",
    "mixing_model = MLP(m2_layers, activation=\"gelu\")\n",
    "m2_checkpoint = torch.load(m2_PATH, map_location=torch.device('cpu'))\n",
    "mixing_model.load_state_dict(m2_checkpoint)\n",
    "mixing_model = mixing_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_input_columns = ['depth', 'day_of_year', 'time_of_day', 'temp_mix03']\n",
    "m3_output_columns = ['temp_conv04']\n",
    "\n",
    "m3_input_column_ix = [data_df.columns.get_loc(column) for column in m3_input_columns]\n",
    "m3_output_column_ix = [data_df.columns.get_loc(column) for column in m3_output_columns]\n",
    "\n",
    "m3_PATH = f\"./../02_training/saved_models/convection_model_time.pth\"\n",
    "m3_layers = [len(m3_input_columns), 32, 32, len(m3_output_columns)]\n",
    "\n",
    "convection_model = MLP(m3_layers, activation=\"gelu\")\n",
    "m3_checkpoint = torch.load(m3_PATH, map_location=torch.device('cpu'))\n",
    "convection_model.load_state_dict(m3_checkpoint)\n",
    "convection_model = convection_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_input_columns = ['depth', 'day_of_year', 'time_of_day', \n",
    "                    'ice', 'snow', 'snowice','temp_conv04']\n",
    "m4_output_columns = ['temp_total05']\n",
    "\n",
    "m4_input_column_ix = [data_df.columns.get_loc(column) for column in m4_input_columns]\n",
    "m4_output_column_ix = [data_df.columns.get_loc(column) for column in m4_output_columns]\n",
    "\n",
    "m4_PATH = f\"./../02_training/saved_models/ice_model_time.pth\"\n",
    "m4_layers = [len(m4_input_columns), 32, 32, len(m4_output_columns)]\n",
    "\n",
    "ice_model = MLP(m4_layers, activation=\"gelu\")\n",
    "m4_checkpoint = torch.load(m4_PATH, map_location=torch.device('cpu'))\n",
    "ice_model.load_state_dict(m4_checkpoint)\n",
    "ice_model = ice_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_temp_columns = ['obs_temp']\n",
    "\n",
    "obs_temp_columns_ix = [data_df.columns.get_loc(column) for column in obs_temp_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_steps = 24\n",
    "# train_data = np.reshape(train_data, (train_data.shape[0]//depth_steps, depth_steps, train_data.shape[1]))\n",
    "# test_data = np.reshape(test_data, (test_data.shape[0]//depth_steps, depth_steps, test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping track of the mean and standard deviations\n",
    "train_mean = scaler.mean_\n",
    "train_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "batch_size = 1000\n",
    "\n",
    "assert batch_size % 25 ==0, \"Batchsize has to be multiple of 25\" \n",
    "\n",
    "train_dataset = DataGenerator(train_data)\n",
    "test_dataset = DataGenerator(test_data)\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    return (((true-pred)**2).mean()**0.5).detach().cpu().numpy()\n",
    "\n",
    "def l2_error(true, pred):\n",
    "    return np.linalg.norm(pred.detach().cpu().numpy() - true.detach().cpu().numpy()) / np.linalg.norm(true.detach().cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping track of the mean and standard deviations\n",
    "\n",
    "input_mean, input_std = train_mean[m1_input_column_ix], train_std[m1_input_column_ix]\n",
    "output_mean, output_std = train_mean[m1_output_column_ix], train_std[m1_output_column_ix]\n",
    "\n",
    "mean_diff = torch.tensor(input_mean[m1_input_column_ix[13]]).to(device)\n",
    "std_diff = torch.tensor(input_std[m1_input_column_ix[13]]).to(device)\n",
    "\n",
    "mean_temp = torch.tensor(input_mean[m1_input_column_ix[14]]).to(device)\n",
    "std_temp = torch.tensor(input_std[m1_input_column_ix[14]]).to(device)\n",
    "\n",
    "mean_out = torch.tensor(output_mean).to(device)\n",
    "std_out = torch.tensor(output_std).to(device)\n",
    "    \n",
    "def implicit_diffusion(diff, temp, dt=3600, dx=1, depth_steps=25):\n",
    "    # de-normalise data\n",
    "    diff = diff * std_diff + mean_diff\n",
    "    diff = diff.view(-1, depth_steps)\n",
    "    \n",
    "    # INPUT DATA FROM PREVIOUS MODULE\n",
    "    t = temp * std_temp + mean_temp # temperature profile from previous module output\n",
    "    t = t.view(-1, depth_steps)\n",
    "    \n",
    "    # IMPLEMENTATION OF CRANK-NICHOLSON SCHEME\n",
    "#     len_t = t.shape[1]\n",
    "    y = torch.zeros((t.shape[0], depth_steps, depth_steps), dtype=torch.float64).to(device)\n",
    "\n",
    "    alpha = (dt/dx**2) * diff\n",
    "\n",
    "    az = - alpha # subdiagonal\n",
    "    bz = 2 * (1 + alpha) # diagonal\n",
    "    cz = - alpha # superdiagonal\n",
    "    \n",
    "    bz[:, 0] = 1\n",
    "    az[:, depth_steps-2] = 0\n",
    "    bz[:, depth_steps-1] = 1\n",
    "    cz[:, 0] = 0\n",
    "    \n",
    "    az = az[:,1:]\n",
    "    cz = cz[:,:-1]\n",
    "\n",
    "    y = torch.diag_embed(bz, offset=0)+torch.diag_embed(az,offset=-1)+torch.diag_embed(cz,offset=1) #slightly efficient way of computing the diagonal matrices\n",
    "    y[:, depth_steps-1, depth_steps-1] = 1\n",
    "    \n",
    "    mn = torch.zeros_like(t)  \n",
    "    mn[:, 0] = t[:, 0]\n",
    "    mn[:,depth_steps-1] = t[:, depth_steps-1]\n",
    "    \n",
    "    mn[:, 1:depth_steps-1] = alpha[:, 1:depth_steps-1]*t[:, :depth_steps-2] + 2 * (1 - alpha[:,1:depth_steps-1])*t[:,1:depth_steps-1] + alpha[:,1:depth_steps-1]*t[:,1:depth_steps-1] #is be same as the loop\n",
    "    \n",
    "    # DERIVED TEMPERATURE OUTPUT FOR NEXT MODULE\n",
    "    proj = torch.linalg.solve(y, mn)\n",
    "\n",
    "    mean, std, var = torch.mean(proj), torch.std(proj), torch.var(proj)\n",
    "    proj = (proj-mean_out)/std_out\n",
    "\n",
    "    proj = proj.to(torch.float32)\n",
    "    proj = proj.view(-1, 1)\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, loader, input_columns, output_columns, train_mean, train_std):\n",
    "    model.eval()\n",
    "    y_ = []\n",
    "    pred_ = []\n",
    "    mean = torch.tensor(train_mean[output_columns]).to(device)\n",
    "    std = torch.tensor(train_std[output_columns]).to(device)\n",
    "    \n",
    "    for x in iter(loader):\n",
    "        inputs, target = x[:, input_columns].to(device).float(), x[:, output_columns].to(device).float()\n",
    "        pred = model(inputs)\n",
    "        target = target * std + mean\n",
    "        pred = pred * std + mean\n",
    "        y_.append(target)\n",
    "        pred_.append(pred)\n",
    "    y_ = torch.cat(y_, dim=0) \n",
    "    pred_ = torch.cat(pred_, dim=0)\n",
    "    \n",
    "    if y_.shape[1]==2:\n",
    "        rmse_temp = rmse(y_[:,1], pred_[:,1])\n",
    "        l2_error_temp = l2_error(y_[:,1], pred_[:,1])\n",
    "    else:\n",
    "        rmse_temp = rmse(y_[:,0], pred_[:,0])\n",
    "        l2_error_temp = l2_error(y_[:,0], pred_[:,0])\n",
    "        \n",
    "    return rmse_temp, l2_error_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m1_input_column_ix)\n",
    "print(m1_input_column_ix[:-1])\n",
    "print(m1_input_column_ix[:-2])\n",
    "print(m1_input_column_ix[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, loader, plot = True):    \n",
    "    heating_model.eval()\n",
    "    heat_diff_model.eval()\n",
    "    mixing_model.eval()\n",
    "    convection_model.eval()\n",
    "    ice_model.eval()\n",
    "\n",
    "    mean = torch.tensor(train_mean[m4_output_column_ix]).float().to(device)\n",
    "    std = torch.tensor(train_std[m4_output_column_ix]).float().to(device)\n",
    "\n",
    "    m1_mean = torch.tensor(train_mean[m1_input_column_ix[-1]]).float().to(device)\n",
    "    m1_std = torch.tensor(train_std[m1_input_column_ix[-1]]).float().to(device)\n",
    "\n",
    "#     depthwise_y_pred = []\n",
    "#     depthwise_y_true = []\n",
    "    y_ = []\n",
    "    y_obs_ = []\n",
    "    pred_ = []\n",
    "        \n",
    "    rmse_models = np.zeros((len(loader), 5))\n",
    "    for ix, x in enumerate(iter(loader)):\n",
    "        x = x.to(device).float()\n",
    "        \n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "            \n",
    "        #model 0\n",
    "        m0_pred = heating_model(m0_input) #predicts diff and temp\n",
    "            \n",
    "        if plot:\n",
    "            m0_y_true = x[:, m0_output_column_ix[0]] * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "            m0_y_pred = m0_pred * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "#             print(m1_y_true.shape, m1_y_pred.shape)\n",
    "#             print(\"True\",m1_y_true)\n",
    "#             print(\"Pred\",m1_y_pred)\n",
    "            rmse_models[ix, 0] = rmse(m0_y_true.squeeze(), m0_y_pred.squeeze())\n",
    "#             print(x[:, m1_output_column_ix[1]])\n",
    "#             print(m1_pred_temp)\n",
    "#             print(criterion(m1_pred_temp, x[:, m1_output_column_ix[1]]))\n",
    "            print(\"RMSE of after m0\", rmse(m0_y_true.flatten(), m0_y_pred.flatten()))\n",
    "\n",
    "        #model 2\n",
    "        m1_input = torch.cat([x[:, m1_input_column_ix[:-1]], m0_pred], dim=-1)\n",
    "        #m1_input = x[:, m1_input_column_ix]\n",
    "            \n",
    "        #model 1\n",
    "        # m1_pred = heat_diff_model(m1_input) #predicts diff and temp\n",
    "        \n",
    "        temp_input = m0_y_pred = m0_pred * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "       # print(temp_input)\n",
    "        \n",
    "        proj = heat_diff_model(m1_input)\n",
    "        \n",
    "        pred = implicit_diffusion(proj, m0_pred)        \n",
    "        m1_pred = pred.to(dtype=torch.float32)\n",
    "        \n",
    "        #print(m1_pred)\n",
    "        \n",
    "        if plot:\n",
    "            m1_y_true = x[:, m1_output_column_ix[0]] * torch.tensor(train_std[m1_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m1_output_column_ix[0]]).to(device)\n",
    "            m1_y_pred = m1_pred * torch.tensor(train_std[m1_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m1_output_column_ix[0]]).to(device)\n",
    "#             print(m1_y_true.shape, m1_y_pred.shape)\n",
    "#             print(\"True\",m1_y_true)\n",
    "#             print(\"Pred\",m1_y_pred)\n",
    "            rmse_models[ix, 1] = rmse(m1_y_true.squeeze(), m1_y_pred.squeeze())\n",
    "#             print(x[:, m1_output_column_ix[1]])\n",
    "#             print(m1_pred_temp)\n",
    "#             print(criterion(m1_pred_temp, x[:, m1_output_column_ix[1]]))\n",
    "            print(\"RMSE of after m1\", rmse(m1_y_true.flatten(), m1_y_pred.flatten()))\n",
    "\n",
    "        #model 2\n",
    "        m2_input = torch.cat([x[:, m2_input_column_ix[:-1]], m1_pred], dim=-1)\n",
    "        m2_pred = mixing_model(m2_input)\n",
    "            \n",
    "        if plot:\n",
    "            m2_y_true = x[:, m2_output_column_ix] * torch.tensor(train_std[m2_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m2_output_column_ix[0]]).to(device)\n",
    "            m2_y_pred = m2_pred * torch.tensor(train_std[m2_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m2_output_column_ix[0]]).to(device)\n",
    "            rmse_models[ix, 2] = rmse(m2_y_true.squeeze(), m2_y_pred.squeeze())\n",
    "            print(\"RMSE of after m2\", rmse(m2_y_true.flatten(), m2_y_pred.flatten()))\n",
    "\n",
    "        #model 3\n",
    "        m3_input = torch.cat([x[:, m3_input_column_ix[:-1]], m2_pred], dim=-1)\n",
    "        m3_pred = convection_model(m3_input)\n",
    "            \n",
    "        if plot:\n",
    "            m3_y_true = x[:, m3_output_column_ix] * torch.tensor(train_std[m3_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m3_output_column_ix[0]]).to(device)\n",
    "            m3_y_pred = m3_pred * torch.tensor(train_std[m3_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m3_output_column_ix[0]]).to(device)\n",
    "            rmse_models[ix, 3] = rmse(m3_y_true.squeeze(), m3_y_pred.squeeze())\n",
    "            print(\"RMSE of after m3\", rmse(m3_y_true.flatten(), m3_y_pred.flatten()))\n",
    "\n",
    "        #model 4\n",
    "        m4_input = torch.cat([x[:, m4_input_column_ix[:-1]], m3_pred], dim=-1)\n",
    "        m4_pred = ice_model(m4_input)\n",
    "            \n",
    "        if plot:\n",
    "            m4_y_true = x[:, m4_output_column_ix] * std + mean\n",
    "            m4_y_pred = m4_pred * std + mean\n",
    "            rmse_models[ix, 4] = rmse(m4_y_true.squeeze(), m4_y_pred.squeeze())\n",
    "            print(\"RMSE of after m4\", rmse(m4_y_true.flatten(), m4_y_pred.flatten()))\n",
    "\n",
    "        y_true = x[:, m4_output_column_ix] * std + mean\n",
    "        y_obs = x[:, obs_temp_columns_ix] * train_std[obs_temp_columns_ix[0]] + train_mean[obs_temp_columns_ix[0]]\n",
    "        pred = m4_pred * std + mean\n",
    "        \n",
    "        y_.append(y_true)\n",
    "        y_obs_.append(y_obs)\n",
    "        pred_.append(pred)\n",
    "\n",
    "    y_ = torch.cat(y_, dim=0)\n",
    "    y_obs_ = torch.cat(y_obs_, dim=0)\n",
    "    pred_ = torch.cat(pred_, dim=0) \n",
    "    \n",
    "    #if plot:   \n",
    "     #        rmse_models = rmse_models.mean(axis=0)\n",
    "     #        plt.figure(figsize=(12,8))\n",
    "     #        plt.plot(rmse_models[:, 0], label=\"RMSE after Heating Model\")\n",
    "     #        plt.plot(rmse_models[:, 1], label=\"RMSE after Heat-Diffusion Model\")\n",
    "     #        plt.plot(rmse_models[:, 2], label=\"RMSE after Mixing Model\")\n",
    "     #        plt.plot(rmse_models[:, 3], label=\"RMSE after Convection Model\")\n",
    "     #        plt.plot(rmse_models[:, 4], label=\"RMSE after Ice Model\")\n",
    "     #        plt.legend(loc=\"upper left\", fontsize=12)\n",
    "     #        plt.xlabel(\"Depth\", fontsize=12)\n",
    "     #        plt.ylabel(\"RMSE\", fontsize=12)\n",
    "     #        plt.grid(\"on\", alpha=0.5)\n",
    "     #        plt.show()\n",
    "    \n",
    "    return pred_, y_, y_obs_, rmse_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y_pred, y_true, depth_steps, time_label, figsize=(20,10)):\n",
    "    time_label = np.array([time[:10] for time in time_label])\n",
    "    time_label = time_label[::depth_steps]\n",
    "    \n",
    "    y_pred = y_pred.flatten().detach().cpu().numpy()\n",
    "    y_true = y_true.flatten().detach().cpu().numpy()\n",
    "    \n",
    "    y_true = np.reshape(y_true, (y_true.shape[0]//depth_steps, depth_steps))\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0]//depth_steps, depth_steps))\n",
    "    \n",
    "    N_pts = 6 # number of points to display on the x-label\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1, figsize=figsize)\n",
    "    sns.heatmap(y_true.T, ax=ax[0], cmap='seismic', vmin=0., vmax=35.)\n",
    "    ax[0].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[0].set_xlabel(\"Time\", fontsize=15)\n",
    "    \n",
    "    xticks_ix = np.array(ax[0].get_xticks()).astype(int)\n",
    "    time_label = time_label[xticks_ix]\n",
    "    nelement = len(time_label)//N_pts\n",
    "    time_label = time_label[::nelement]\n",
    "    ax[0].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[0].set_xticklabels(time_label, rotation=0)   \n",
    "#     ax[0].xaxis.set_major_locator(plt.MultipleLocator(100))\n",
    "    ax[0].collections[0].colorbar.set_label(\"Actual Temperature\")\n",
    "    \n",
    "    sns.heatmap(y_pred.T, ax=ax[1], cmap='seismic', vmin=0., vmax=35.)\n",
    "    ax[1].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[1].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[1].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[1].set_xticklabels(time_label, rotation=0)\n",
    "    ax[1].collections[0].colorbar.set_label(\"Predicted Temperature\")\n",
    "    \n",
    "    sns.heatmap(np.abs(y_pred.T-y_true.T), ax=ax[2], cmap='viridis')\n",
    "    ax[2].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[2].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[2].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[2].set_xticklabels(time_label, rotation=0)\n",
    "    \n",
    "    ax[2].collections[0].colorbar.set_label(\"Absolute Error\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(heating_model,heat_diff_model, mixing_model, convection_model, ice_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics( heat_diff_model, train_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(heat_diff_model, test_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(mixing_model, train_loader, m2_input_column_ix, m2_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(mixing_model, test_loader, m2_input_column_ix, m2_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(convection_model, train_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(convection_model, test_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(ice_model, train_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(ice_model, test_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "# decay_rate = 0.1\n",
    "# decay_steps = 500\n",
    "\n",
    "params = list(heating_model.parameters()) + list(heat_diff_model.parameters()) + list(mixing_model.parameters()) + list(convection_model.parameters()) + list(ice_model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=lr, \n",
    "                             betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=decay_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze_model(heating_model)\n",
    "freeze_model(heating_model)\n",
    "freeze_model(heat_diff_model)\n",
    "unfreeze_model(mixing_model)\n",
    "freeze_model(convection_model)\n",
    "freeze_model(ice_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on Observed Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating_model.train()\n",
    "heat_diff_model.train()\n",
    "mixing_model.train()\n",
    "convection_model.train()\n",
    "ice_model.train()\n",
    "\n",
    "n_epochs = 500\n",
    "\n",
    "# mean and standard dev of \"temp_total04\" of model 4 output\n",
    "mean_out4 = torch.tensor(train_mean[m4_output_column_ix[0]]).float().to(device)\n",
    "std_out4 = torch.tensor(train_std[m4_output_column_ix[0]]).float().to(device)\n",
    "\n",
    "mean_obs = torch.tensor(train_mean[obs_temp_columns_ix[0]]).float().to(device)\n",
    "std_obs = torch.tensor(train_std[obs_temp_columns_ix[0]]).float().to(device)\n",
    "\n",
    "# mean and standard dev of \"input_temp\" of model 1 input\n",
    "m1_mean = torch.tensor(train_mean[m1_input_column_ix[-1]]).float().to(device)\n",
    "m1_std = torch.tensor(train_std[m1_input_column_ix[-1]]).float().to(device)\n",
    "\n",
    "train_loss = []\n",
    "LOSS_m0 = []\n",
    "LOSS_m1 = []\n",
    "LOSS_m2 = []\n",
    "LOSS_m3 = []\n",
    "LOSS_m4 = []\n",
    "\n",
    "for it in tqdm(range(n_epochs)):\n",
    "    loss_epoch = 0\n",
    "    loss_epoch_m0 = 0\n",
    "    loss_epoch_m1 = 0\n",
    "    loss_epoch_m2 = 0\n",
    "    loss_epoch_m3 = 0\n",
    "    loss_epoch_m4 = 0\n",
    "    for ix, x in enumerate(iter(train_loader)):\n",
    "        x = x.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "      #  m1_input = x[:, m1_input_column_ix]\n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "        \n",
    "        #model 0\n",
    "        m0_pred = heating_model(m0_input) #predicts diff and temp\n",
    "        loss_m0 = criterion(m0_pred, x[:, m0_output_column_ix])\n",
    "\n",
    "        #model 1\n",
    "        m1_input = torch.cat([x[:, m1_input_column_ix[:-1]], m0_pred], dim=-1)\n",
    "        # m1_pred = heat_diff_model(m1_input)\n",
    "        \n",
    "        proj = heat_diff_model(m1_input)\n",
    "        pred = implicit_diffusion(proj, m0_pred)        \n",
    "        m1_pred = pred.to(dtype=torch.float32)\n",
    "                \n",
    "        loss_m1 = criterion(m1_pred, x[:, m1_output_column_ix])\n",
    "        \n",
    "        #m1_pred = heat_diff_model(m1_input) #predicts diff and temp\n",
    "        #m1_pred_temp = m1_pred[:,1:2]\n",
    "            \n",
    "        #loss_m1 = criterion(m1_pred_temp, x[:, m1_output_column_ix[1]].unsqueeze(1))\n",
    "\n",
    "        #model 2\n",
    "        m2_input = torch.cat([x[:, m2_input_column_ix[:-1]], m1_pred], dim=-1)\n",
    "        m2_pred = mixing_model(m2_input)\n",
    "            \n",
    "        loss_m2 = criterion(m2_pred, x[:, m2_output_column_ix])\n",
    "\n",
    "        #model 3\n",
    "        m3_input = torch.cat([x[:, m3_input_column_ix[:-1]], m2_pred], dim=-1)\n",
    "        m3_pred = convection_model(m3_input)\n",
    "\n",
    "        loss_m3 = criterion(m3_pred, x[:, m3_output_column_ix])\n",
    "\n",
    "        #model 4\n",
    "        m4_input = torch.cat([x[:, m4_input_column_ix[:-1]], m3_pred], dim=-1)\n",
    "        m4_pred = ice_model(m4_input)\n",
    "        \n",
    "        obs_temp_true = x[:, obs_temp_columns_ix] * std_obs + mean_obs\n",
    "        obs_temp_true_norm = (obs_temp_true - mean_out4)/std_out4\n",
    "        \n",
    "        loss_m4 = criterion(m4_pred, obs_temp_true_norm)\n",
    "\n",
    "        #loss = (loss_m0 + loss_m1 + loss_m2 + loss_m3 + loss_m4)\n",
    "        \n",
    "        loss = loss_m4\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss.item()\n",
    "        loss_epoch_m0 += loss_m0.item()\n",
    "        loss_epoch_m1 += loss_m1.item()\n",
    "        loss_epoch_m2 += loss_m2.item()\n",
    "        loss_epoch_m3 += loss_m3.item()\n",
    "        loss_epoch_m4 += loss_m4.item()\n",
    "    \n",
    "    loss_epoch = loss_epoch/len(train_loader)\n",
    "    loss_epoch_m0 = loss_epoch_m0/len(train_loader)\n",
    "    loss_epoch_m1 = loss_epoch_m1/len(train_loader)\n",
    "    loss_epoch_m2 = loss_epoch_m2/len(train_loader)\n",
    "    loss_epoch_m3 = loss_epoch_m3/len(train_loader)\n",
    "    loss_epoch_m4 = loss_epoch_m4/len(train_loader)\n",
    "    \n",
    "    train_loss.append(loss_epoch)\n",
    "    LOSS_m0.append(loss_epoch_m0)\n",
    "    LOSS_m1.append(loss_epoch_m1)\n",
    "    LOSS_m2.append(loss_epoch_m2)\n",
    "    LOSS_m3.append(loss_epoch_m3)\n",
    "    LOSS_m4.append(loss_epoch_m4)\n",
    "    if it % 50 == 0:\n",
    "        print(f\"Epoch : {it}, Train_loss: {train_loss[-1]}, Loss m0: {LOSS_m0[-1]}, Loss m1: {LOSS_m1[-1]}, Loss m2: {LOSS_m2[-1]}, Loss m3: {LOSS_m3[-1]}, Loss m4: {LOSS_m4[-1]}\")\n",
    "    \n",
    "    #plot the loss_m1, m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss, label = \"Train Loss\")\n",
    "plt.plot(LOSS_m0, label = \"Loss M0\")\n",
    "plt.plot(LOSS_m1, label = \"Loss M1\")\n",
    "plt.plot(LOSS_m2, label = \"Loss M2\")\n",
    "plt.plot(LOSS_m3, label = \"Loss M3\")\n",
    "plt.plot(LOSS_m4, label = \"Loss M4\")\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=15)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"./saved_models/mixing_model_finetuned.pth\"\n",
    "torch.save(mixing_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Evaluation After FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, mixing_model, convection_model, ice_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(heat_diff_model, train_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(heat_diff_model, test_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(mixing_model, train_loader, m2_input_column_ix, m2_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(mixing_model, test_loader, m2_input_column_ix, m2_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(convection_model, train_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(convection_model, test_loader, m3_input_column_ix, m3_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(ice_model, train_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(ice_model, test_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Model Similarity after Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_similarity(model1, model2):\n",
    "    weight1 = []\n",
    "    weight2 = []\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        weight1.append(param1.detach().clone().flatten())\n",
    "        weight2.append(param2.detach().clone().flatten())\n",
    "    weight1 = torch.cat(weight1, dim=0)\n",
    "    weight2 = torch.cat(weight2, dim=0)\n",
    "    \n",
    "    #Cosine Similarity\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
    "    cos_sim = cos(weight1, weight2)\n",
    "    \n",
    "    #L2 norm\n",
    "    l2 = torch.norm((weight1-weight2), p='fro', dim=0)\n",
    "    \n",
    "    return cos_sim, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating_model_init = MLP(m0_layers, activation=\"gelu\")\n",
    "heating_model_init.load_state_dict(m0_checkpoint)\n",
    "heating_model_init = heating_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(heating_model_init, heating_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_diff_model_init = MLP(m1_layers, activation=\"gelu\")\n",
    "heat_diff_model_init.load_state_dict(m1_checkpoint)\n",
    "heat_diff_model_init = heat_diff_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(heat_diff_model_init, heat_diff_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixing_model_init = MLP(m2_layers, activation=\"gelu\")\n",
    "mixing_model_init.load_state_dict(m2_checkpoint)\n",
    "mixing_model_init = mixing_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(mixing_model_init, mixing_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convection_model_init = MLP(m3_layers, activation=\"gelu\")\n",
    "convection_model_init.load_state_dict(m3_checkpoint)\n",
    "convection_model_init = convection_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(convection_model_init, convection_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_model_init = MLP(m4_layers, activation=\"gelu\")\n",
    "ice_model_init.load_state_dict(m4_checkpoint)\n",
    "ice_model_init = ice_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(ice_model_init, ice_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all models again and compute finetuned diffusivity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze diffusion model and finetune it on projected diffusivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
