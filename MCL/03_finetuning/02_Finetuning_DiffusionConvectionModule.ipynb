{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does not make sense\n",
    "\n",
    "Cannot be finetuned with all 4 losses. The term 1 and term 4 are contradictory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ladwi\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layers, activation=\"relu\", init=\"xavier\"):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = torch.nn.Tanh()\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"Unspecified activation type\")\n",
    "        \n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "        if init==\"xavier\":\n",
    "            self.xavier_init_weights()\n",
    "        elif init==\"kaiming\":\n",
    "            self.kaiming_init_weights()\n",
    "    \n",
    "    def xavier_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Xavier Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def kaiming_init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            print(\"Initializing Network with Kaiming Initialization..\")\n",
    "            for m in self.layers.modules():\n",
    "                if hasattr(m, 'weight'):\n",
    "                    nn.init.kaiming_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.0)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "    \n",
    "class DataGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"./../02_training/all_data_lake_modeling_in_time.csv\")\n",
    "time = data_df['time']\n",
    "data_df = data_df.drop(columns=['time'])\n",
    "data_df\n",
    "display(data_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frac = 0.60\n",
    "depth_steps = 50\n",
    "number_days = len(data_df)//depth_steps\n",
    "n_obs = int(number_days*training_frac)*depth_steps\n",
    "print(f\"Number of days total: {number_days}\")\n",
    "print(f\"Number of training points: {n_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_df.values\n",
    "\n",
    "train_data = data[:n_obs]\n",
    "test_data = data[n_obs:]\n",
    "\n",
    "train_time = time[:n_obs]\n",
    "test_time = time[n_obs:]\n",
    "\n",
    "#performing normalization on all the columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in ['temp_heat01']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_input_columns = ['depth', 'AirTemp_degC', 'Longwave_Wm-2', 'Latent_Wm-2', 'Sensible_Wm-2', 'Shortwave_Wm-2',\n",
    "                'lightExtinct_m-1','Area_m2', \n",
    "                 'day_of_year', 'time_of_day', 'ice', 'snow', 'snowice', 'temp_initial00']\n",
    "m0_output_columns = ['temp_heat01']\n",
    "\n",
    "m0_input_column_ix = [data_df.columns.get_loc(column) for column in m0_input_columns]\n",
    "m0_output_column_ix = [data_df.columns.get_loc(column) for column in m0_output_columns]\n",
    "\n",
    "m0_PATH =  f\"./../02_training/saved_models/heating_model_time.pth\"\n",
    "m0_layers = [len(m0_input_columns), 32, 32, len(m0_output_columns)]\n",
    "\n",
    "heating_model = MLP(m0_layers, activation=\"gelu\")\n",
    "m0_checkpoint = torch.load(m0_PATH, map_location=torch.device('cpu'))\n",
    "heating_model.load_state_dict(m0_checkpoint)\n",
    "heating_model = heating_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1_input_columns = ['depth', 'Area_m2', 'Uw',\n",
    "                 'buoyancy', 'day_of_year', 'time_of_day',  'ice', 'snow', 'snowice','diffusivity', 'temp_total05']\n",
    "m1_output_columns = ['temp_conv04']\n",
    "\n",
    "m1_input_column_ix = [data_df.columns.get_loc(column) for column in m1_input_columns]\n",
    "m1_output_column_ix = [data_df.columns.get_loc(column) for column in m1_output_columns]\n",
    "\n",
    "m1_PATH = f\"./../02_training/saved_models/diffusionconvection_model_time.pth\"\n",
    "m1_layers = [len(m1_input_columns), 32, 32, len(m1_output_columns)]\n",
    "\n",
    "heat_diff_model = MLP(m1_layers, activation=\"gelu\")\n",
    "m1_checkpoint = torch.load(m1_PATH, map_location=torch.device('cpu'))\n",
    "heat_diff_model.load_state_dict(m1_checkpoint)\n",
    "heat_diff_model = heat_diff_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_input_columns = ['depth', 'day_of_year', 'time_of_day', 'ice', 'snow', 'snowice',\n",
    "                   'temp_heat01']\n",
    "m4_output_columns = ['temp_total05']\n",
    "\n",
    "m4_input_column_ix = [data_df.columns.get_loc(column) for column in m4_input_columns]\n",
    "m4_output_column_ix = [data_df.columns.get_loc(column) for column in m4_output_columns]\n",
    "\n",
    "m4_PATH = f\"./../02_training/saved_models/ice_model_time.pth\"\n",
    "m4_layers = [len(m4_input_columns), 32, 32, len(m4_output_columns)]\n",
    "\n",
    "ice_model = MLP(m4_layers, activation=\"gelu\")\n",
    "m4_checkpoint = torch.load(m4_PATH, map_location=torch.device('cpu'))\n",
    "ice_model.load_state_dict(m4_checkpoint)\n",
    "ice_model = ice_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_temp_columns = ['obs_temp']\n",
    "\n",
    "obs_temp_columns_ix = [data_df.columns.get_loc(column) for column in obs_temp_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_steps = 24\n",
    "# train_data = np.reshape(train_data, (train_data.shape[0]//depth_steps, depth_steps, train_data.shape[1]))\n",
    "# test_data = np.reshape(test_data, (test_data.shape[0]//depth_steps, depth_steps, test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping track of the mean and standard deviations\n",
    "train_mean = scaler.mean_\n",
    "train_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "batch_size = 1000\n",
    "\n",
    "#assert batch_size % 25 ==0, \"Batchsize has to be multiple of 25\" \n",
    "\n",
    "train_dataset = DataGenerator(train_data)\n",
    "test_dataset = DataGenerator(test_data)\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    return (((true-pred)**2).mean()**0.5).detach().cpu().numpy()\n",
    "\n",
    "def l2_error(true, pred):\n",
    "    return np.linalg.norm(pred.detach().cpu().numpy() - true.detach().cpu().numpy()) / np.linalg.norm(true.detach().cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, loader, input_columns, output_columns, train_mean, train_std):\n",
    "    model.eval()\n",
    "    y_ = []\n",
    "    pred_ = []\n",
    "    mean = torch.tensor(train_mean[output_columns]).to(device)\n",
    "    std = torch.tensor(train_std[output_columns]).to(device)\n",
    "    \n",
    "    for x in iter(loader):\n",
    "        inputs, target = x[:, input_columns].to(device).float(), x[:, output_columns].to(device).float()\n",
    "        pred = model(inputs)\n",
    "        target = target * std + mean\n",
    "        pred = pred * std + mean\n",
    "        y_.append(target)\n",
    "        pred_.append(pred)\n",
    "    y_ = torch.cat(y_, dim=0) \n",
    "    pred_ = torch.cat(pred_, dim=0)\n",
    "    \n",
    "    if y_.shape[1]==2:\n",
    "        rmse_temp = rmse(y_[:,1], pred_[:,1])\n",
    "        l2_error_temp = l2_error(y_[:,1], pred_[:,1])\n",
    "    else:\n",
    "        rmse_temp = rmse(y_[:,0], pred_[:,0])\n",
    "        l2_error_temp = l2_error(y_[:,0], pred_[:,0])\n",
    "        \n",
    "    return rmse_temp, l2_error_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollout_predictions(heating_model, heat_diff_model, ice_model, loader, plot = True):    \n",
    "    heating_model.eval()\n",
    "    heat_diff_model.eval()\n",
    "    ice_model.eval()\n",
    "\n",
    "    mean = torch.tensor(train_mean[m1_output_column_ix]).float().to(device)\n",
    "    std = torch.tensor(train_std[m1_output_column_ix]).float().to(device)\n",
    "\n",
    "    m1_mean = torch.tensor(train_mean[m1_input_column_ix[-1]]).float().to(device)\n",
    "    m1_std = torch.tensor(train_std[m1_input_column_ix[-1]]).float().to(device)\n",
    "\n",
    "#     depthwise_y_pred = []\n",
    "#     depthwise_y_true = []\n",
    "    y_ = []\n",
    "    y_obs_ = []\n",
    "    pred_ = []\n",
    "        \n",
    "    rmse_models = np.zeros((len(loader), 5))\n",
    "    for ix, x in enumerate(iter(loader)):\n",
    "        x = x.to(device).float()\n",
    "        \n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "            \n",
    "        #model 0\n",
    "        m0_pred = heating_model(m0_input) #predicts diff and temp\n",
    "            \n",
    "        if plot:\n",
    "            m0_y_true = x[:, m0_output_column_ix[0]] * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "            m0_y_pred = m0_pred * torch.tensor(train_std[m0_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m0_output_column_ix[0]]).to(device)\n",
    "#             print(m1_y_true.shape, m1_y_pred.shape)\n",
    "#             print(\"True\",m1_y_true)\n",
    "#             print(\"Pred\",m1_y_pred)\n",
    "            rmse_models[ix, 0] = rmse(m0_y_true.squeeze(), m0_y_pred.squeeze())\n",
    "#             print(x[:, m1_output_column_ix[1]])\n",
    "#             print(m1_pred_temp)\n",
    "#             print(criterion(m1_pred_temp, x[:, m1_output_column_ix[1]]))\n",
    "            print(\"RMSE of after m0\", rmse(m0_y_true.flatten(), m0_y_pred.flatten()))\n",
    "\n",
    "        #model 2\n",
    "        m4_input = torch.cat([x[:, m4_input_column_ix[:-1]], m0_pred], dim=-1)\n",
    "\n",
    "        m4_pred = ice_model(m4_input)\n",
    "\n",
    "        \n",
    "        #print(m1_pred)\n",
    "        \n",
    "        if plot:\n",
    "            m4_y_true = x[:, m4_output_column_ix[0]] * torch.tensor(train_std[m4_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m4_output_column_ix[0]]).to(device)\n",
    "            m4_y_pred = m4_pred * torch.tensor(train_std[m4_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m4_output_column_ix[0]]).to(device)\n",
    "#             print(m1_y_true.shape, m1_y_pred.shape)\n",
    "#             print(\"True\",m1_y_true)\n",
    "#             print(\"Pred\",m1_y_pred)\n",
    "            rmse_models[ix, 1] = rmse(m4_y_true.squeeze(), m4_y_pred.squeeze())\n",
    "#             print(x[:, m1_output_column_ix[1]])\n",
    "#             print(m1_pred_temp)\n",
    "#             print(criterion(m1_pred_temp, x[:, m1_output_column_ix[1]]))\n",
    "            print(\"RMSE of after m1\", rmse(m4_y_true.flatten(), m4_y_pred.flatten()))\n",
    "\n",
    "\n",
    "        #model 3\n",
    "        m1_input = torch.cat([x[:, m1_input_column_ix[:-1]], m4_pred], dim=-1)\n",
    "        m1_pred = heat_diff_model(m1_input)\n",
    "            \n",
    "        if plot:\n",
    "            m1_y_true = x[:, m1_output_column_ix] * torch.tensor(train_std[m1_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m1_output_column_ix[0]]).to(device)\n",
    "            m1_y_pred = m1_pred * torch.tensor(train_std[m1_output_column_ix[0]]).to(device) + torch.tensor(train_mean[m1_output_column_ix[0]]).to(device)\n",
    "            rmse_models[ix, 3] = rmse(m1_y_true.squeeze(), m1_y_pred.squeeze())\n",
    "            print(\"RMSE of after m3\", rmse(m1_y_true.flatten(), m1_y_pred.flatten()))\n",
    "\n",
    "\n",
    "\n",
    "        y_true = x[:, m1_output_column_ix] * std + mean\n",
    "        y_obs = x[:, obs_temp_columns_ix] * train_std[obs_temp_columns_ix[0]] + train_mean[obs_temp_columns_ix[0]]\n",
    "        pred = m1_pred * std + mean\n",
    "        \n",
    "        y_.append(y_true)\n",
    "        y_obs_.append(y_obs)\n",
    "        pred_.append(pred)\n",
    "\n",
    "    y_ = torch.cat(y_, dim=0)\n",
    "    y_obs_ = torch.cat(y_obs_, dim=0)\n",
    "    pred_ = torch.cat(pred_, dim=0) \n",
    "    \n",
    "    #if plot:   \n",
    "     #        rmse_models = rmse_models.mean(axis=0)\n",
    "     #        plt.figure(figsize=(12,8))\n",
    "     #        plt.plot(rmse_models[:, 0], label=\"RMSE after Heating Model\")\n",
    "     #        plt.plot(rmse_models[:, 1], label=\"RMSE after Heat-Diffusion Model\")\n",
    "     #        plt.plot(rmse_models[:, 2], label=\"RMSE after Mixing Model\")\n",
    "     #        plt.plot(rmse_models[:, 3], label=\"RMSE after Convection Model\")\n",
    "     #        plt.plot(rmse_models[:, 4], label=\"RMSE after Ice Model\")\n",
    "     #        plt.legend(loc=\"upper left\", fontsize=12)\n",
    "     #        plt.xlabel(\"Depth\", fontsize=12)\n",
    "     #        plt.ylabel(\"RMSE\", fontsize=12)\n",
    "     #        plt.grid(\"on\", alpha=0.5)\n",
    "     #        plt.show()\n",
    "    \n",
    "    return pred_, y_, y_obs_, rmse_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y_pred, y_true, depth_steps, time_label, figsize=(20,10)):\n",
    "    time_label = np.array([time[:10] for time in time_label])\n",
    "    time_label = time_label[::depth_steps]\n",
    "    \n",
    "    y_pred = y_pred.flatten().detach().cpu().numpy()\n",
    "    y_true = y_true.flatten().detach().cpu().numpy()\n",
    "    \n",
    "    y_true = np.reshape(y_true, (y_true.shape[0]//depth_steps, depth_steps))\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0]//depth_steps, depth_steps))\n",
    "    \n",
    "    N_pts = 6 # number of points to display on the x-label\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1, figsize=figsize)\n",
    "    sns.heatmap(y_true.T, ax=ax[0], cmap='seismic', vmin=0., vmax=35.)\n",
    "    ax[0].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[0].set_xlabel(\"Time\", fontsize=15)\n",
    "    \n",
    "    xticks_ix = np.array(ax[0].get_xticks()).astype(int)\n",
    "    time_label = time_label[xticks_ix]\n",
    "    nelement = len(time_label)//N_pts\n",
    "    time_label = time_label[::nelement]\n",
    "    ax[0].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[0].set_xticklabels(time_label, rotation=0)   \n",
    "#     ax[0].xaxis.set_major_locator(plt.MultipleLocator(100))\n",
    "    ax[0].collections[0].colorbar.set_label(\"Actual Temperature\")\n",
    "    \n",
    "    sns.heatmap(y_pred.T, ax=ax[1], cmap='seismic', vmin=0., vmax=35.)\n",
    "    ax[1].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[1].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[1].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[1].set_xticklabels(time_label, rotation=0)\n",
    "    ax[1].collections[0].colorbar.set_label(\"Predicted Temperature\")\n",
    "    \n",
    "    sns.heatmap(np.abs(y_pred.T-y_true.T), ax=ax[2], cmap='viridis')\n",
    "    ax[2].set_ylabel(\"Depth\", fontsize=15)\n",
    "    ax[2].set_xlabel(\"Time\", fontsize=15)\n",
    "    ax[2].xaxis.set_major_locator(plt.MaxNLocator(N_pts))\n",
    "    ax[2].set_xticklabels(time_label, rotation=0)\n",
    "    \n",
    "    ax[2].collections[0].colorbar.set_label(\"Absolute Error\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, ice_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(heating_model,heat_diff_model, ice_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics( heat_diff_model, train_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(heat_diff_model, test_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(ice_model, train_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(ice_model, test_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "# decay_rate = 0.1\n",
    "# decay_steps = 500\n",
    "\n",
    "params = list(heating_model.parameters()) + list(heat_diff_model.parameters())+ list(ice_model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=lr, \n",
    "                             betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=decay_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze_model(heating_model)\n",
    "freeze_model(heating_model)\n",
    "unfreeze_model(heat_diff_model)\n",
    "freeze_model(ice_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on Observed Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating_model.train()\n",
    "heat_diff_model.train()\n",
    "ice_model.train()\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "# mean and standard dev of \"temp_total04\" of model 4 output\n",
    "mean_out1 = torch.tensor(train_mean[m1_output_column_ix[0]]).float().to(device)\n",
    "std_out1 = torch.tensor(train_std[m1_output_column_ix[0]]).float().to(device)\n",
    "\n",
    "mean_obs = torch.tensor(train_mean[obs_temp_columns_ix[0]]).float().to(device)\n",
    "std_obs = torch.tensor(train_std[obs_temp_columns_ix[0]]).float().to(device)\n",
    "\n",
    "# mean and standard dev of \"input_temp\" of model 1 input\n",
    "m1_mean = torch.tensor(train_mean[m1_input_column_ix[-1]]).float().to(device)\n",
    "m1_std = torch.tensor(train_std[m1_input_column_ix[-1]]).float().to(device)\n",
    "\n",
    "train_loss = []\n",
    "LOSS_m0 = []\n",
    "LOSS_m1 = []\n",
    "LOSS_m4 = []\n",
    "\n",
    "for it in tqdm(range(n_epochs)):\n",
    "    loss_epoch = 0\n",
    "    loss_epoch_m0 = 0\n",
    "    loss_epoch_m1 = 0\n",
    "    loss_epoch_m4 = 0\n",
    "    for ix, x in enumerate(iter(train_loader)):\n",
    "        x = x.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "      #  m1_input = x[:, m1_input_column_ix]\n",
    "        m0_input = x[:, m0_input_column_ix]\n",
    "        \n",
    "        #model 0\n",
    "        m0_pred = heating_model(m0_input) #predicts diff and temp\n",
    "        loss_m0 = criterion(m0_pred, x[:, m0_output_column_ix])\n",
    "\n",
    "        #model 1\n",
    "        m4_input = torch.cat([x[:, m4_input_column_ix[:-1]], m0_pred], dim=-1)\n",
    "        # m1_pred = heat_diff_model(m1_input)\n",
    "        \n",
    "        m4_pred = ice_model(m4_input)\n",
    "        loss_m4 = criterion(m4_pred, x[:, m4_output_column_ix])\n",
    "        \n",
    "        #m1_pred = heat_diff_model(m1_input) #predicts diff and temp\n",
    "        #m1_pred_temp = m1_pred[:,1:2]\n",
    "            \n",
    "        #loss_m1 = criterion(m1_pred_temp, x[:, m1_output_column_ix[1]].unsqueeze(1))\n",
    "\n",
    "        #model 3\n",
    "        m1_input = torch.cat([x[:, m1_input_column_ix[:-1]], m4_pred], dim=-1)\n",
    "        m1_pred = heat_diff_model(m1_input)\n",
    "\n",
    "        \n",
    "        obs_temp_true = x[:, obs_temp_columns_ix] * std_obs + mean_obs\n",
    "        obs_temp_true_norm = (obs_temp_true - mean_out1)/std_out1\n",
    "        \n",
    "        loss_m1 = criterion(m1_pred, obs_temp_true_norm)\n",
    "\n",
    "        #loss = (loss_m0 + loss_m1 + loss_m2 + loss_m3 + loss_m4)\n",
    "        \n",
    "        loss = loss_m1\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss.item()\n",
    "        loss_epoch_m0 += loss_m0.item()\n",
    "        loss_epoch_m1 += loss_m1.item()\n",
    "        loss_epoch_m4 += loss_m4.item()\n",
    "    \n",
    "    loss_epoch = loss_epoch/len(train_loader)\n",
    "    loss_epoch_m0 = loss_epoch_m0/len(train_loader)\n",
    "    loss_epoch_m1 = loss_epoch_m1/len(train_loader)\n",
    "    loss_epoch_m4 = loss_epoch_m4/len(train_loader)\n",
    "    \n",
    "    train_loss.append(loss_epoch)\n",
    "    LOSS_m0.append(loss_epoch_m0)\n",
    "    LOSS_m1.append(loss_epoch_m1)\n",
    "    LOSS_m4.append(loss_epoch_m4)\n",
    "    if it % 50 == 0:\n",
    "        print(f\"Epoch : {it}, Train_loss: {train_loss[-1]}, Loss m0: {LOSS_m0[-1]}, Loss m1: {LOSS_m1[-1]}, Loss m4: {LOSS_m4[-1]}\")\n",
    "    \n",
    "    #plot the loss_m1, m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss, label = \"Train Loss\")\n",
    "plt.plot(LOSS_m0, label = \"Loss M0\")\n",
    "plt.plot(LOSS_m1, label = \"Loss M1\")\n",
    "#plt.plot(LOSS_m2, label = \"Loss M2\")\n",
    "#plt.plot(LOSS_m3, label = \"Loss M3\")\n",
    "plt.plot(LOSS_m4, label = \"Loss M4\")\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.xlabel(\"Epoch\", fontsize=15)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Evaluation After FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_y_pred, train_y_true, train_y_obs, train_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, ice_model, train_loader, plot = True)\n",
    "\n",
    "train_rmse = rmse(train_y_pred.flatten(), train_y_true.flatten())\n",
    "train_rmse_obs = rmse(train_y_pred.flatten(), train_y_obs.flatten())\n",
    "train_l2 = l2_error(train_y_pred.flatten(), train_y_true.flatten())\n",
    "\n",
    "print(f\"Train RMSE Simulated: {train_rmse}\")\n",
    "print(f\"Train RMSE Observed Temp: {train_rmse_obs}\")\n",
    "print(f\"Train L2 Error: {train_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {train_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_true, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(train_y_pred, train_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_y_true, test_y_obs, test_rmse_models = get_rollout_predictions(heating_model, heat_diff_model, ice_model, test_loader, plot = True)\n",
    "\n",
    "test_rmse = rmse(test_y_pred.flatten(), test_y_true.flatten())\n",
    "test_rmse_obs = rmse(test_y_pred.flatten(), test_y_obs.flatten())\n",
    "test_l2 = l2_error(test_y_pred.flatten(), test_y_true.flatten())\n",
    "\n",
    "print(f\"Test RMSE Simulated: {test_rmse}\")\n",
    "print(f\"Test RMSE Observed Temp: {test_rmse_obs}\")\n",
    "print(f\"test L2 Error: {test_l2}\")\n",
    "print(f\"The RMSEs after each modelling stage: {test_rmse_models.mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_true, depth_steps, test_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_y_pred, test_y_obs, depth_steps, train_time, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ALL Models individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(heat_diff_model, train_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(heat_diff_model, test_loader, m1_input_column_ix, m1_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_l2_err = compute_metrics(ice_model, train_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Train RMSE: {train_rmse}, Train L2 Error: {train_l2_err}\")\n",
    "\n",
    "test_rmse, test_l2_err = compute_metrics(ice_model, test_loader, m4_input_column_ix, m4_output_column_ix, train_mean, train_std)\n",
    "print(f\"Test RMSE: {test_rmse}, Train L2 Error: {test_l2_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Model Similarity after Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_similarity(model1, model2):\n",
    "    weight1 = []\n",
    "    weight2 = []\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        weight1.append(param1.detach().clone().flatten())\n",
    "        weight2.append(param2.detach().clone().flatten())\n",
    "    weight1 = torch.cat(weight1, dim=0)\n",
    "    weight2 = torch.cat(weight2, dim=0)\n",
    "    \n",
    "    #Cosine Similarity\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-8)\n",
    "    cos_sim = cos(weight1, weight2)\n",
    "    \n",
    "    #L2 norm\n",
    "    l2 = torch.norm((weight1-weight2), p='fro', dim=0)\n",
    "    \n",
    "    return cos_sim, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating_model_init = MLP(m0_layers, activation=\"gelu\")\n",
    "heating_model_init.load_state_dict(m0_checkpoint)\n",
    "heating_model_init = heating_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(heating_model_init, heating_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_diff_model_init = MLP(m1_layers, activation=\"gelu\")\n",
    "heat_diff_model_init.load_state_dict(m1_checkpoint)\n",
    "heat_diff_model_init = heat_diff_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(heat_diff_model_init, heat_diff_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_model_init = MLP(m4_layers, activation=\"gelu\")\n",
    "ice_model_init.load_state_dict(m4_checkpoint)\n",
    "ice_model_init = ice_model_init.to(device)\n",
    "\n",
    "cos_sim, l2_dist = compute_model_similarity(ice_model_init, ice_model)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n",
    "print(f\"L2 Norm: {l2_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all models again and compute finetuned diffusivity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze diffusion model and finetune it on projected diffusivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"./saved_models/diffusionconvection_model_finetuned.pth\"\n",
    "torch.save(heat_diff_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
